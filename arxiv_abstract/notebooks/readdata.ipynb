{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd621904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the arxiv-summarization dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f00d6",
   "metadata": {},
   "source": [
    "# Read hg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eec1c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "Dataset info:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 203037\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6436\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6440\n",
      "    })\n",
      "})\n",
      "\n",
      "train split:\n",
      "  Number of examples: 203037\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when comp...\n",
      "    abstract: additive models play an important role in semiparametric statistics . \n",
      " this paper gives learning rates for regularized kernel based methods for additive models . \n",
      " these learning rates compare favour...\n",
      "\n",
      "validation split:\n",
      "  Number of examples: 6436\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: the interest in anchoring phenomena and phenomena in confined nematic liquid crystals has largely been driven by their potential use in liquid crystal display devices . \n",
      " the twisted nematic liquid cr...\n",
      "    abstract: we study the phase behavior of a nematic liquid crystal confined between a flat substrate with strong anchoring and a patterned substrate whose structure and local anchoring strength we vary . by firs...\n",
      "\n",
      "test split:\n",
      "  Number of examples: 6440\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: for about 20 years the problem of properties of short - term changes of solar activity has been considered extensively . \n",
      " many investigators studied the short - term periodicities of the various indi...\n",
      "    abstract: the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \n",
      " the correlative analysis indicates negative correlation for the ...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "\n",
    "# Explore the dataset structure\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"\\nDataset info:\")\n",
    "print(dataset)\n",
    "\n",
    "# Check the structure of a sample from each split\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"\\n{split_name} split:\")\n",
    "    print(f\"  Number of examples: {len(dataset[split_name])}\")\n",
    "    if len(dataset[split_name]) > 0:\n",
    "        print(f\"  Features: {dataset[split_name].features}\")\n",
    "        print(f\"  First example keys: {list(dataset[split_name][0].keys())}\")\n",
    "        print(f\"  First example:\")\n",
    "        for key, value in dataset[split_name][0].items():\n",
    "            if isinstance(value, str) and len(value) > 200:\n",
    "                print(f\"    {key}: {value[:200]}...\")\n",
    "            else:\n",
    "                print(f\"    {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5a908",
   "metadata": {},
   "source": [
    "## write train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create oumi dataset for the entire training set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"arxiv_summarization_train_oumi.jsonl\"\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "print(f\"Converting {len(train_dataset)} training examples to oumi format...\")\n",
    "print(f\"Output file: {output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "# This avoids the overhead of creating Conversation/Message objects\n",
    "conversations_written = 0\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(train_dataset, desc=\"Processing\", total=len(train_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict (faster than creating objects)\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9d9ff",
   "metadata": {},
   "source": [
    "# Write validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80847c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 6436 validation examples to oumi format...\n",
      "Output file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation: 100%|██████████| 6436/6436 [00:00<00:00, 9249.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully created oumi dataset with 6436 conversations\n",
      "  File: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n",
      "  File size: 210.91 MB\n"
     ]
    }
   ],
   "source": [
    "# Create oumi dataset for validation set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_output_path = output_dir / \"arxiv_summarization_validation_oumi.jsonl\"\n",
    "\n",
    "val_dataset = dataset['validation']\n",
    "print(f\"Converting {len(val_dataset)} validation examples to oumi format...\")\n",
    "print(f\"Output file: {val_output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "conversations_written = 0\n",
    "with open(val_output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(val_dataset, desc=\"Processing validation\", total=len(val_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {val_output_path}\")\n",
    "print(f\"  File size: {val_output_path.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d08473",
   "metadata": {},
   "source": [
    "## write test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08faf176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 6440 test examples to oumi format...\n",
      "Output file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 6440/6440 [00:00<00:00, 9217.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully created oumi dataset with 6440 conversations\n",
      "  File: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n",
      "  File size: 211.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Create oumi dataset for test set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_output_path = output_dir / \"arxiv_summarization_test_oumi.jsonl\"\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "print(f\"Converting {len(test_dataset)} test examples to oumi format...\")\n",
    "print(f\"Output file: {test_output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "conversations_written = 0\n",
    "with open(test_output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(test_dataset, desc=\"Processing test\", total=len(test_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {test_output_path}\")\n",
    "print(f\"  File size: {test_output_path.stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec3f88",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a190e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "# Import oumi DatasetAnalyzer to get token length for Qwen3-8B\n",
    "from oumi.core.analyze import DatasetAnalyzer\n",
    "from oumi.core.configs import AnalyzeConfig, DatasetSource, SampleAnalyzerParams\n",
    "from oumi.datasets import TextSftJsonLinesDataset\n",
    "from oumi.core.types.conversation import Conversation, Message, Role\n",
    "import json\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc22e01",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f4efb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading oumi dataset and running analysis...\n",
      "Dataset file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_oumi.jsonl\n",
      "[2025-11-12 16:40:12,131][oumi][rank0][pid:33744][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "Dataset loaded: 203037 conversations\n"
     ]
    }
   ],
   "source": [
    "# Analyze the full training dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure output_path is defined (in case cell 3 wasn't run)\n",
    "if 'output_path' not in locals():\n",
    "    output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "    output_path = output_dir / \"arxiv_summarization_train_oumi.jsonl\"\n",
    "\n",
    "# Check if file exists\n",
    "if not Path(output_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "train_oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(output_path))\n",
    "\n",
    "print(f\"Dataset loaded: {len(train_oumi_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6306702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DatasetAnalyzer (this may take a while for the full dataset)...\n",
      "[2025-11-12 16:40:13,278][oumi][rank0][pid:33744][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'Qwen/Qwen3-8B'.\n",
      "[2025-11-12 16:40:13,279][oumi.utils.analysis_utils][rank0][pid:33744][MainThread][INFO]][analysis_utils.py:57] Built tokenizer for model: Qwen/Qwen3-8B\n",
      "[2025-11-12 16:40:13,280][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:142] Using provided dataset 'arxiv_summarization_train' with 203037 conversations\n",
      "[2025-11-12 16:40:13,280][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:275] Initialized sample analyzer: length\n",
      "[2025-11-12 16:40:13,280][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:301] Starting analysis of dataset: arxiv_summarization_train\n",
      "[2025-11-12 16:40:13,282][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:302] Using 1 sample analyzers: ['length']\n",
      "[2025-11-12 16:40:13,283][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:325] Analyzing 20000 of 203037 conversations\n",
      "[2025-11-12 16:40:13,283][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:374] Converting conversation dataset with 203037 items\n",
      "[2025-11-12 16:40:13,283][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:381] Limiting analysis to first 20000 items (dataset has 203037 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting arxiv_summarization_train to DataFrames: 100%|██████████| 20000/20000 [00:04<00:00, 4640.84item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:40:18,566][oumi][rank0][pid:33744][MainThread][WARNING]][dataframe_analyzer.py:101] Analyzer length failed: No text fields found in the DataFrame for length analysis. Please ensure your schema specifies columns withcontent_type='text'and that those columns exist in the DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (136905 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Analysis completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_train\",\n",
    "    sample_count=20000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer (this may take a while for the full dataset)...\")\n",
    "analyzer = DatasetAnalyzer(config, dataset=train_oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    analyzer.analyze_dataset()\n",
    "    print(\"✓ Analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "        # The analysis should still have completed - the error is only in summary generation\n",
    "        # Try to access the results anyway\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results (should be available even if summary generation failed)\n",
    "analysis_df = analyzer.analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a14466f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "839.5079106846358"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analysis_df.query(\"role == 'assistant'\")['text_content_token_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e38be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:44:34,727][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:435] Query 'text_content_token_count < 7500 & role == 'user'' returned 10402 rows\n",
      "[2025-11-12 16:44:34,732][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:580] Filtered dataset: 10402 conversations out of 203037 total\n",
      "Filtered dataset size: 10402 conversations\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "train_filtered_dataset = analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "\n",
    "print(f\"Filtered dataset size: {len(train_filtered_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take first 10k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_train_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(10000, len(train_filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = train_filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0297b",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5116173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation oumi dataset and running analysis...\n",
      "Dataset file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n",
      "[2025-11-12 16:04:29,196][oumi][rank0][pid:33744][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "Validation dataset loaded: 6436 conversations\n",
      "Running DatasetAnalyzer on validation set...\n",
      "[2025-11-12 16:04:29,651][oumi][rank0][pid:33744][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'Qwen/Qwen3-8B'.\n",
      "[2025-11-12 16:04:29,652][oumi.utils.analysis_utils][rank0][pid:33744][MainThread][INFO]][analysis_utils.py:57] Built tokenizer for model: Qwen/Qwen3-8B\n",
      "[2025-11-12 16:04:29,653][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:142] Using provided dataset 'arxiv_summarization_validation' with 6436 conversations\n",
      "[2025-11-12 16:04:29,653][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:275] Initialized sample analyzer: length\n",
      "[2025-11-12 16:04:29,654][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:301] Starting analysis of dataset: arxiv_summarization_validation\n",
      "[2025-11-12 16:04:29,655][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:302] Using 1 sample analyzers: ['length']\n",
      "[2025-11-12 16:04:29,655][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:325] Analyzing 2000 of 6436 conversations\n",
      "[2025-11-12 16:04:29,656][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:374] Converting conversation dataset with 6436 items\n",
      "[2025-11-12 16:04:29,656][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:381] Limiting analysis to first 2000 items (dataset has 6436 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting arxiv_summarization_validation to DataFrames: 100%|██████████| 2000/2000 [00:00<00:00, 4276.16item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:04:30,184][oumi][rank0][pid:33744][MainThread][WARNING]][dataframe_analyzer.py:101] Analyzer length failed: No text fields found in the DataFrame for length analysis. Please ensure your schema specifies columns withcontent_type='text'and that those columns exist in the DataFrame.\n",
      "✓ Validation analysis completed successfully\n",
      "\n",
      "✓ Validation analysis complete!\n",
      "  Total messages analyzed: 4000\n",
      "\n",
      "================================================================================\n",
      "Validation Set Summary Statistics by Role:\n",
      "================================================================================\n",
      "\n",
      "USER messages (2000 total):\n",
      "  Token count - Mean: 8948.6, Median: 7461.0, Min: 245, Max: 55237\n",
      "  Character count - Mean: 33872.9, Median: 29023.5\n",
      "  Word count - Mean: 6035.0, Median: 5166.0\n",
      "\n",
      "ASSISTANT messages (2000 total):\n",
      "  Token count - Mean: 199.7, Median: 192.0, Min: 52, Max: 697\n",
      "  Character count - Mean: 958.7, Median: 937.0\n",
      "  Word count - Mean: 161.4, Median: 157.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze the validation dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Validation dataset path\n",
    "val_output_path = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\")\n",
    "\n",
    "# Check if file exists\n",
    "if not val_output_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {val_output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading validation oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {val_output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "val_oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(val_output_path))\n",
    "\n",
    "print(f\"Validation dataset loaded: {len(val_oumi_dataset)} conversations\")\n",
    "\n",
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "val_config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_validation\",\n",
    "    sample_count=2000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer on validation set...\")\n",
    "val_analyzer = DatasetAnalyzer(val_config, dataset=val_oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    val_analyzer.analyze_dataset()\n",
    "    print(\"✓ Validation analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results\n",
    "val_analysis_df = val_analyzer.analysis_df\n",
    "\n",
    "print(f\"\\n✓ Validation analysis complete!\")\n",
    "print(f\"  Total messages analyzed: {len(val_analysis_df)}\")\n",
    "\n",
    "# Find columns with metrics\n",
    "token_cols = [col for col in val_analysis_df.columns if 'token_count' in col.lower()]\n",
    "char_cols = [col for col in val_analysis_df.columns if 'char_count' in col.lower()]\n",
    "word_cols = [col for col in val_analysis_df.columns if 'word_count' in col.lower()]\n",
    "\n",
    "# Display summary statistics by role\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Validation Set Summary Statistics by Role:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for role in ['user', 'assistant']:\n",
    "    role_rows = val_analysis_df[val_analysis_df['role'] == role]\n",
    "    if len(role_rows) > 0:\n",
    "        print(f\"\\n{role.upper()} messages ({len(role_rows)} total):\")\n",
    "        if token_cols:\n",
    "            token_col = token_cols[0]\n",
    "            print(f\"  Token count - Mean: {role_rows[token_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[token_col].median():.1f}, \"\n",
    "                  f\"Min: {role_rows[token_col].min()}, \"\n",
    "                  f\"Max: {role_rows[token_col].max()}\")\n",
    "        if char_cols:\n",
    "            char_col = char_cols[0]\n",
    "            print(f\"  Character count - Mean: {role_rows[char_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[char_col].median():.1f}\")\n",
    "        if word_cols:\n",
    "            word_col = word_cols[0]\n",
    "            print(f\"  Word count - Mean: {role_rows[word_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[word_col].median():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c24986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:07:32,031][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:435] Query 'text_content_token_count < 7500 & role == 'user'' returned 1008 rows\n",
      "[2025-11-12 16:07:32,033][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:580] Filtered dataset: 1008 conversations out of 6436 total\n",
      "Filtered dataset size: 1008 conversations\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "val_filtered_dataset = val_analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "print(f\"Filtered dataset size: {len(val_filtered_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e012a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving first 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_filtered_10k.jsonl...\n",
      "✓ Successfully saved 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_filtered_10k.jsonl\n",
      "  File size: 18.96 MB\n"
     ]
    }
   ],
   "source": [
    "# Take first 1k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_val_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(1000, len(val_filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = val_filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15484a6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8587a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test oumi dataset and running analysis...\n",
      "Dataset file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n",
      "[2025-11-12 16:08:47,406][oumi][rank0][pid:33744][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "Test dataset loaded: 6440 conversations\n",
      "Running DatasetAnalyzer on test set...\n",
      "[2025-11-12 16:08:47,803][oumi][rank0][pid:33744][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'Qwen/Qwen3-8B'.\n",
      "[2025-11-12 16:08:47,803][oumi.utils.analysis_utils][rank0][pid:33744][MainThread][INFO]][analysis_utils.py:57] Built tokenizer for model: Qwen/Qwen3-8B\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:142] Using provided dataset 'arxiv_summarization_test' with 6440 conversations\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:275] Initialized sample analyzer: length\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:301] Starting analysis of dataset: arxiv_summarization_test\n",
      "[2025-11-12 16:08:47,805][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:302] Using 1 sample analyzers: ['length']\n",
      "[2025-11-12 16:08:47,805][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:325] Analyzing 2000 of 6440 conversations\n",
      "[2025-11-12 16:08:47,806][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:374] Converting conversation dataset with 6440 items\n",
      "[2025-11-12 16:08:47,807][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:381] Limiting analysis to first 2000 items (dataset has 6440 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting arxiv_summarization_test to DataFrames: 100%|██████████| 2000/2000 [00:00<00:00, 6007.58item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:08:48,348][oumi][rank0][pid:33744][MainThread][WARNING]][dataframe_analyzer.py:101] Analyzer length failed: No text fields found in the DataFrame for length analysis. Please ensure your schema specifies columns withcontent_type='text'and that those columns exist in the DataFrame.\n",
      "✓ Test analysis completed successfully\n",
      "\n",
      "✓ Test analysis complete!\n",
      "  Total messages analyzed: 4000\n",
      "\n",
      "================================================================================\n",
      "Test Set Summary Statistics by Role:\n",
      "================================================================================\n",
      "\n",
      "USER messages (2000 total):\n",
      "  Token count - Mean: 8772.7, Median: 7324.0, Min: 282, Max: 67349\n",
      "  Character count - Mean: 33309.4, Median: 28449.0\n",
      "  Word count - Mean: 5937.6, Median: 5080.0\n",
      "\n",
      "ASSISTANT messages (2000 total):\n",
      "  Token count - Mean: 203.5, Median: 198.0, Min: 54, Max: 608\n",
      "  Character count - Mean: 973.0, Median: 962.5\n",
      "  Word count - Mean: 164.2, Median: 161.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze the test dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Test dataset path\n",
    "test_output_path = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\")\n",
    "\n",
    "# Check if file exists\n",
    "if not test_output_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {test_output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading test oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {test_output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "test_oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(test_output_path))\n",
    "\n",
    "print(f\"Test dataset loaded: {len(test_oumi_dataset)} conversations\")\n",
    "\n",
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "test_config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_test\",\n",
    "    sample_count=2000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer on test set...\")\n",
    "test_analyzer = DatasetAnalyzer(test_config, dataset=test_oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    test_analyzer.analyze_dataset()\n",
    "    print(\"✓ Test analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results\n",
    "test_analysis_df = test_analyzer.analysis_df\n",
    "\n",
    "print(f\"\\n✓ Test analysis complete!\")\n",
    "print(f\"  Total messages analyzed: {len(test_analysis_df)}\")\n",
    "\n",
    "# Find columns with metrics\n",
    "token_cols = [col for col in test_analysis_df.columns if 'token_count' in col.lower()]\n",
    "char_cols = [col for col in test_analysis_df.columns if 'char_count' in col.lower()]\n",
    "word_cols = [col for col in test_analysis_df.columns if 'word_count' in col.lower()]\n",
    "\n",
    "# Display summary statistics by role\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Test Set Summary Statistics by Role:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for role in ['user', 'assistant']:\n",
    "    role_rows = test_analysis_df[test_analysis_df['role'] == role]\n",
    "    if len(role_rows) > 0:\n",
    "        print(f\"\\n{role.upper()} messages ({len(role_rows)} total):\")\n",
    "        if token_cols:\n",
    "            token_col = token_cols[0]\n",
    "            print(f\"  Token count - Mean: {role_rows[token_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[token_col].median():.1f}, \"\n",
    "                  f\"Min: {role_rows[token_col].min()}, \"\n",
    "                  f\"Max: {role_rows[token_col].max()}\")\n",
    "        if char_cols:\n",
    "            char_col = char_cols[0]\n",
    "            print(f\"  Character count - Mean: {role_rows[char_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[char_col].median():.1f}\")\n",
    "        if word_cols:\n",
    "            word_col = word_cols[0]\n",
    "            print(f\"  Word count - Mean: {role_rows[word_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[word_col].median():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f27b39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:09:12,514][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:435] Query 'text_content_token_count < 7500 & role == 'user'' returned 1026 rows\n",
      "[2025-11-12 16:09:12,516][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:580] Filtered dataset: 1026 conversations out of 6440 total\n",
      "Filtered dataset size: 1026 conversations\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "test_filtered_dataset = test_analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "print(f\"Filtered dataset size: {len(test_filtered_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "befd5f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving first 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_filtered_10k.jsonl...\n",
      "✓ Successfully saved 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_filtered_10k.jsonl\n",
      "  File size: 19.09 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Take first 1k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_test_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(1000, len(test_filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = test_filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eecf73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4d4c31",
   "metadata": {},
   "source": [
    "# System instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af02355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    evaluate_summary,\n",
    "    evaluate_summaries_batch,\n",
    "    display_text,\n",
    "    display_message,\n",
    "    load_conversations,\n",
    "    client,\n",
    "    JUDGE_SYSTEM_INSTRUCTION,\n",
    "    JUDGE_PROMPT_TEMPLATE_WITH_REQUEST_AND_RESPONSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d24b4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_filtered_10k.jsonl\"\n",
    "val_path =   \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_filtered_10k.jsonl\"\n",
    "test_path =  \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_filtered_10k.jsonl\"\n",
    "# train_distilled_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_abstract_train_gpt5mini_think2.jsonl\"\n",
    "train_conversations = load_conversations(train_path)\n",
    "val_conversations = load_conversations(val_path)\n",
    "test_conversations = load_conversations(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc25cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': {'request': 'for the hybrid monte carlo algorithm ( hmc)@xcite , often used to study quantum chromodynamics ( qcd ) on the lattice , one is interested in efficient numerical time integration schemes which are optimal in terms of computational costs per trajectory for a given acceptance rate . high order \\n numerical methods allow the use of larger step sizes , but demand a larger computational effort per step ; low order schemes do not require such large computational costs per step , but need more steps per trajectory . \\n so there is a need to balance these opposing effects . \\n omelyan integration schemes @xcite of a force - gradient type have proved to be an efficient choice , since it is easy to obtain higher order schemes that demand a small additional computational effort . \\n these schemes use higher - order information from force - gradient terms to both increase the convergence of the method and decrease the size of the leading error coefficient . other ideas to achieve better efficiency for numerical time integrators are given by multirate or nested approaches . \\n these schemes do not increase the order but reduce the computational costs per path by recognizing the different dynamical time - scales generated by different parts of the action . \\n slow forces , which are usually expensive to evaluate , need only to be sampled at low frequency while fast forces which are usually cheap to evaluate need to be sampled at a high frequency . a natural way to inherit the advantages from both force - gradient type schemes and multirate approaches would be to combine these two ideas .    previously , we studied the behavior of the adapted nested force - gradient scheme for the example of the @xmath0-body problem @xcite and would like to learn more about their usefulness for lattice field theory calculations . due to the huge computational effort required for qcd simulations , \\n it is natural to attempt an intermediate step first . \\n we chose the model problem of quantum electrodynamics ( qed ) in two dimensions , the schwinger model @xcite , since it is well - suited as a test case for new concepts and ideas which can be subsequently applied to more computationally demanding problems @xcite . \\n as a lattice quantum field theory , it has many of the properties of more sophisticated models such as qcd , for example the numerical cost is still dominated by the fermion part of the action . the fact that this model , with far fewer degrees of freedom , does not require such large computational effort makes it the perfect choice for testing purposes . \\n we compare the behavior of numerical time integration schemes currently used for hmc @xcite with the nested force - gradient integrator @xcite and the adapted version introduced in @xcite . \\n we investigate the computational costs needed to perform numerical calculations , as well as the effort required to achieve a satisfactory acceptance rate during the hmc evolution . \\n our goal is to find a numerical scheme for the hmc algorithm which would provide a sufficiently high acceptance rate while not drastically increasing the simulation time . \\n the paper is organized as follows . in section 2 \\n we give a short overview of the hmc algorithm and numerical schemes for time integration , which are used in hmc . in section 3 \\n we present the 2-dimensional schwinger model and introduce the idea of the force - gradient approach and the resulting novel class of numerical schemes . \\n section 4 is devoted to the results of a comparison between widely used algorithms and the new approach and section 5 draws our conclusion . \\n in this section we provide a general overview of the hmc algorithm @xcite to introduce our novel integrator . \\n we also present some standard numerical time integrating methods used in hmc , as well state - of - the - art numerical schemes , which we later compare by applying them to the two - dimensional schwinger model .      in the hybrid monte carlo algorithm , \\n the quantum lattice field theory is embedded in a higher - dimensional classical system through the introduction of a fictitious ( simulation ) time @xcite . \\n the gauge field @xmath1 is associated with its ( fictitious ) conjugate momenta @xmath2 , and the classical system is described by the hamiltonian , @xmath3 + { \\\\mathcal{b}}[u],\\\\ ] ] where @xmath4 $ ] and @xmath5 $ ] represent the kinetic and potential energy respectively .    for a given configuration @xmath1 , \\n a new configuration @xmath6 is generated by performing an hmc update @xmath7 , which consists of two steps :    * * molecular dynamics trajectory : * evolve the gauge fields @xmath1 , elements of a lie group , and the momenta @xmath2 , elements of the corresponding lie algebra , in a fictitious computer time @xmath8 according to hamilton s equations of motions @xmath9 since analytical solutions are not available in general , numerical methods must be used to solve the system of eqn .  . \\n the discrete updates of @xmath1 and @xmath2 with an integration step @xmath10 are @xmath11 leading to a first - order approximation at time @xmath12 . \\n since the momenta @xmath2 are elements of lie algebra , we have an additive update of @xmath2 . \\n on the other hand , the links @xmath1 must be elements of the lie group , therefore an exponential update is used for @xmath1 to preserve the underlying group structure . * * metropolis step : * accept or reject the new configuration @xmath13 with probability @xmath14 where @xmath15 .      in this paper \\n we are concerned with numerical time integration schemes , which preserve the fundamental properties of geometric integration , time - reversibility and volume - preservation . \\n all numerical schemes presented below possess these necessary properties . \\n * basic schemes : * well - known , commonly used integration schemes in molecular dynamics are given by    * the leap - frog method , a 3-stage composition scheme of the discrete updates defined above : @xmath16 * and a 5-stage extension widely used in qcd computations : @xmath17    * force gradient schemes : * force - gradient schemes increase accuracy by using additional information from the force gradient term @xmath18 , with @xmath19 defining lie brackets . the 5-stage force - gradient scheme proposed by omelyan et \\n al @xcite is the simplest ; @xmath20 here we also test the modification of the force - gradient method proposed in @xcite , where the force - gradient term @xmath21 is approximated via a taylor expansion . \\n an extension is given by the 11-stage decomposition @xcite , recently implemented as the integrator in the open source code openqcd as one of the standard options @xcite \\n @xmath22where @xmath23 , @xmath24 , @xmath25 and @xmath26 are parameters from equation ( 71 ) in ref . \\n @xcite .    * \\n nested schemes : * qed and qcd problems usually lead to hamiltonians with the following fine structure @xmath27 + { \\\\mathcal{b}}_{1}[u]+ { \\\\mathcal{b}}_{2}[u],\\\\ ] ] where the action of the system can be split into two parts : a fast action @xmath28 such as the gauge action , and a slow part @xmath29 , for example , the fermion action . \\n this allows us to apply the idea of multirate schemes ( an idea known as nested integration in physics literature)@xcite in order to reduce the computational effort . at \\n first we consider the nested version of the leap - frog method @xmath30 where the inner cheaper system @xmath31+{\\\\mathcal{b}}_{1}[u]$ ] is solved by @xmath32 with @xmath33 being a number of iterations for the fast part of the action . \\n our main goal is to compare the above - mentioned methods with more elaborated nested schemes : in @xcite , a similar 5-stage decomposition scheme has been recently introduced : @xmath34    a nested version of , which has been used in  @xcite reads @xmath35 where @xmath36 with @xmath37 and @xmath38 . in the limit @xmath39 \\n we have @xmath40 . \\n note that this approach uses force - gradient information at all levels , i.e. , the high computational cost of high order schemes appears at all levels . \\n one may overcome this problem by using schemes of different order at the different levels without losing the effective high order of the overall multirate scheme . \\n for the latter , we include appropriate force gradient information as we explain in the following for the case of two time levels , where the gauge action plays the role of the fast and cheap part , and the fermionic action plays the role of the slow and expensive part . \\n the reasoning is as follows : if one uses the 5-stage sexton - weingarten integrator of second order for the slow action , and approximates the fast action by @xmath41 leap - frog steps of step size @xmath42 , the error of the overall multirate scheme will be of order @xmath43 . with the use of force gradient information only at the slowest level it is possible to cancel the leading error term of order @xmath44 . \\n as @xmath45 usually holds in the multirate setting , the overall order is then given by the leading error term of order @xmath46 , i.e. , the scheme has an effective order of four . \\n one example for such a scheme for problems of type is given by the 5-stage nested force - gradient scheme introduced in @xcite @xmath47 to summarize , the adapted scheme   differs from the original one   in two perspectives :    * the force gradient scheme for the fast action is replaced by a leap - frog scheme . * \\n only the part @xmath48 of the full force gradient @xmath49 is needed to gain the effective order of four . \\n the numerical schemes - and - are second order convergent schemes . methods - and - have the fourth order of convergence . \\n we do not consider integrators of higher order than four since the computational costs are too high . \\n the schemes of the same convergence order differ from each other by the number of stages ( updates of momenta and links per time step ) . \\n usually methods with more stages have smaller leading error coefficients and therefore have better accuracy , but higher computational costs . \\n we would like to determine which integrator would represent the best compromise between high accuracy and computational efficiency \\n .    we will apply all these numerical integration schemes  to the two - dimensional schwinger model . \\n the most challenging task from the theoretical point of view is to derive the force - gradient term @xmath21 . in the next section \\n we introduce the schwinger model and explain how to obtain the force - gradient term . \\n the 2 dimensional schwinger model is defined by the following hamiltonian function @xmath50 = \\\\frac{1}{2}\\\\sum_{n=1,\\\\mu=1}^{v,2 } p_{n,\\\\mu}^2 + s_g[u ] + s_f[u].}\\\\ ] ] with @xmath51 the volume of the lattice . unlike qcd , where @xmath52 and @xmath53 , for this qed problem , the links @xmath1 are the elements of the lie group @xmath54 and the momenta @xmath55 belong to @xmath56 , which represents the lie algebra of the group @xmath54 . \\n this makes this test example very cheap in terms of the computational time . \\n this together with the fact that the schwinger model also shares many of the features of qcd simulations , makes the schwinger model an excellent test example when considering numerical integrators : a fast dynamics given by the computationally cheap gauge part @xmath57 $ ] of the action demanding small step sizes , and a slow dynamics given by the computationally expensive fermion part @xmath58 $ ] allowing large step sizes . \\n the pure gauge part of the action @xmath59 sums up over all plaquettes @xmath60 in the two - dimensional lattice with @xmath61 and is given by @xmath62 the links @xmath1 can be written in the form @xmath63 and connect the sites @xmath0 and @xmath64 on the lattice ; @xmath65 $ ] , @xmath66 , @xmath67 @xmath68 are respectively space and time directions and @xmath69 is a coupling constant . \\n note that from now on we will set the lattice spacing @xmath70 . \\n the fermion part of the action @xmath71 is given by @xmath72 where @xmath26 is a complex pseudofermion field . here , @xmath73 denotes the wilson  dirac operator given by @xmath74 where @xmath75 are the pauli matrices @xmath76 @xmath77 is the mass parameter and the kronecker delta @xmath78 acts on the pseudofermion field by @xmath79 with @xmath80 the pseudofermion field , a vector in the two - dimensional spinor space taking values at each lattice point @xmath0 . in order to proceed with the numerical integration we need to obtain the force @xmath81 and the force gradient term @xmath21 . \\n the force term @xmath82 with respect to the link @xmath83 is given by the first derivative of the action @xmath84 and can be written as @xmath85 since the numerical schemes  use the multi - rate approach , the shifts in the momenta updates are split on @xmath86 and @xmath87 and we can consider them separately . \\n the force terms @xmath86 and @xmath87 are obtained by differentiation over @xmath54 group elements , which for the schwinger model is the standard differentiation .    the force associated with link @xmath88 from the gauge action \\n is given by @xmath89 the force term of the fermion part is given by @xmath90 \\\\,\\\\ ] ] where vectors @xmath91 and @xmath92 are given @xmath93    for the numerical methods and we need to find the force gradient term @xmath94 with respect to the link @xmath83 . in case of the schwinger model this term reads @xmath95    for simplicity we decompose the force gradient term in four parts @xmath96 this decomposition is also useful since the numerical integrator only uses the term @xmath97 by construction . \\n as shown in @xcite , to obtain the fourth order convergent scheme from the second order convergent method we must eliminate the leading error term , which is exactly represented by @xmath97 . for completeness we discuss all 4 parts below . \\n the @xmath98 part of the force - gradient term is @xmath99 \\\\end{aligned}\\\\ ] ] with the set of plaquettes @xmath100 then by using the vectors @xmath101 defined in we obtain the @xmath102 piece of the force - gradient term given by @xmath103 . } \\n \\\\end{aligned}\\\\ ] ] the second derivative of the fermion action is @xmath104 \\\\xi + } \\\\nonumber \\\\\\\\ & & { 2 \\\\operatorname{re } \\\\chi^\\\\dagger \\\\frac{\\\\partial d}{\\\\partial q_\\\\mu(n ) } ( d^\\\\dagger d)^{-1 } \\\\frac{\\\\partial d^\\\\dagger}{\\\\partial q_\\\\nu(m ) } \\\\chi \\\\ , , } \\\\nonumber \\\\\\\\ & & { = 2 \\\\operatorname{re } \\\\left [ z_{1,m,\\\\nu}^\\\\dagger \\\\frac{\\\\partial d}{\\\\partial q_{\\\\mu}(n ) } \\\\xi +   \\\\chi^\\\\dagger \\n \\\\frac{\\\\partial d}{\\\\partial q_{\\\\mu}(n ) } d^{-1 } w_{2,m,\\\\nu } -   \\\\chi^\\\\dagger   \\\\frac{\\\\partial^2 d}{\\\\partial q_{\\\\nu}(m ) \\\\partial q_{\\\\mu}(n ) } \\\\xi +   \\\\chi^\\\\dagger \\n \\\\frac{\\\\partial d}{\\\\partial q_{\\\\mu}(n ) } d^{-1 } z_{1,m,\\\\nu}\\\\right ] } \\\\nonumber \\\\\\\\ & & { = 2 \\\\textrm{re } \\\\left [ z_{1,m,\\\\nu}^\\\\dagger w_{2,n,\\\\mu } +   w_{1,n,\\\\mu}^\\\\dagger z_{2,m,\\\\nu } -   \\\\chi^\\\\dagger   \\\\frac{\\\\partial^2 d}{\\\\partial q_{\\\\nu}(m ) \\\\partial q_{\\\\mu}(n ) } \\\\xi \\\\right ] } \\\\end{aligned}\\\\ ] ] in terms of the vectors @xmath105 and @xmath92 defined in . \\n now the fields @xmath106 and @xmath107 are given by @xmath108 with @xmath109    in order to calculate @xmath110 and @xmath111 it is possible to perform the summation of @xmath112 before the inversions of @xmath73 and @xmath113 to get @xmath114 and @xmath115 which save @xmath116 additional inversions for the force gradient terms . \\n it follows for the force gradient term @xmath111 @xmath117\\\\ ] ] with @xmath118 + z_1 \\\\right ) \\\\ , . \\n \\\\end{aligned}\\\\ ] ] the expression for @xmath110 can be obtained from the one for @xmath111 by replacing in and the vector @xmath119 with @xmath120 defined in . \\n it is important to mention that the computationally most demanding part of the numerical integration of the schwinger model and quantum field theory in general is the inverse of the dirac operator @xmath121 . \\n every momenta update , which includes fermion action requires 2 inversions of the dirac operator , the addition of the force - gradient term @xmath21 requires 4 more inversions . \\n therefore leap - frog based methods and need 4 computations of @xmath121 per time step ; schemes and 6 times ; force - gradient based methods 8 for and , 10 for and the 11 stage method has 12 inversions of the dirac operator . since we use the multi - rate approach for schemes , and , which leads generally to fewer macro time steps needed than for the standard schemes we expect the integrator will be the most efficient choice among the methods considered . in the next section we present numerical tests of this prediction . \\n in this section we apply the numerical integrators  to compute the molecular dynamics step for the schwinger model when studied with the hmc algorithm . \\n we consider a @xmath122 by @xmath122 lattice with a coupling constant @xmath123 and mass @xmath124 . \\n the parameters were taken from @xcite and correspond to the scaling variable @xmath125 defined in @xcite.we have chosen them to simulate close to the scaling limit with light fermions and also to increase the impact of the fermion part of the action . \\n we use one thermalised gauge configuration . for each integrator and value of the step - size \\n we generate @xmath126 independent sets of momenta and integrate the equations of motion on a trajectory of length @xmath127 . \\n we compute the absolute error @xmath128 and estimate its statistical error from the standard deviation . \\n also the parameter @xmath33 is chosen in such a way to make micro step size to be @xmath129 times smaller than the macro step size @xmath10 . \\n figure [ fig:1 ] presents the comparison between the numerical integrators  . \\n it shows the absolute error @xmath128 versus the step - size of the numerical scheme . here \\n the multi - rate schemes , , and outperform their standard versions as expected . \\n also it is easy to see that the scheme has the best accuracy and the nested force - gradient method just slightly edges the adapted nested force - gradient scheme . \\n figure [ fig:2 ] presents the cpu time , required for the proposed integrators \\n , versus the achieved accuracy . \\n we can observe that the nested force - gradient method and adapted nested force- gradient method show much better results in terms of a computational efficiency than the integrators and ; and even compared to the 11 stage scheme . \\n here we can see that the modification of proposed in @xcite also performs better than its original version . \\n it shows almost similar computational costs as nested versions of the force - gradient approach - , since it has the same number of @xmath121  ( see table [ tab:1 ] ) . \\n but it is less efficient because the proposed nested approach is more precise . \\n .step - sizes and number of inversions of @xmath73 per step and per trajectory for acceptance rate of 90% [ cols=\"^,^,^,^,^\",options=\"header \" , ]     table [ tab:1 ] shows the number of inversions of the dirac operator @xmath73 , which is needed to reach 90% acceptance rate of the hmc . since @xmath121 is the most computationally demanding part it is important to see how many of these inversions are required per each trajectory . from table \\n [ tab:1 ] it easy to see that the adapted nested force - gradient method and nested force - gradient method need the least number of @xmath121 per trajectory to reach the chosen acceptance rate @xmath130 . \\n we can also claim that methods and have a potential to perform even better with respect to the computational effort in the case of lattice qcd problems , since the impact of the fermion action and the computational time to obtain the inversion of the dirac operator @xmath73 is much more significant . \\n we presented the nested force - gradient approach and its adapted version applied to a model problem in quantum field theory , the two - dimensional schwinger model . \\n the derivation of the force - gradient terms was given and the schwinger model was introduced . \\n nested force - gradient schemes seem to be an optimal choice with relatively high convergence order and low computational effort . \\n also it would be possible to improve the algorithm by measuring the poisson brackets of the shadow hamiltonian of the proposed integrator and then tuning the set of optimal parameters , e.  g. micro and macro step sizes . \\n + in future work we will apply this approach to the hmc algorithm for numerical integration in lattice qcd . here \\n we expect the adapted nested - force gradient scheme to outperform the original one , if we further partition the action into more than two parts , by using techniques to factorize the fermion determinant : less force - gradient information is needed for the most expensive action , and only leap - frog steps are needed for the high frequency parts of the action . \\n this work is part of project b5 within the sfb / transregio 55 _ hadronenphysik mit gitter - qcd _ funded by dfg ( deutsche forschungsgemeinschaft ) . \\n s.  duane , a.d . \\n kennedy , b.j . \\n pendleton , d.  roweth , hybrid monte carlo , phys . \\n b195 ( 1987 ) , pp . \\n e.  hairer , c.  lubich , g.  wanner , geometric numerical integration : structure - preserving algorithms for ordinary differential equations , springer , berlin , 2002 . \\n omelyan , i.m . \\n mryglod , r.  folk , symplectic analytically integrable decomposition algorithms : classification , derivation , and application to molecular dynamics , quantum and celestial mechanics , comput . \\n 151 ( 2003 ) , pp .', 'response': 'we study a novel class of numerical integrators , the adapted nested force - gradient schemes , used within the molecular dynamics step of the hybrid monte carlo ( hmc ) algorithm . \\n we test these methods in the schwinger model on the lattice , a well known benchmark problem . \\n we derive the analytical basis of nested force - gradient type methods and demonstrate the advantage of the proposed approach , namely reduced computational costs compared with other numerical integration schemes in hmc .'}}\n"
     ]
    }
   ],
   "source": [
    "def convert_conversations_to_content_format(conversations):\n",
    "    \"\"\"\n",
    "    Convert conversations from messages format to content format.\n",
    "    \n",
    "    Args:\n",
    "        conversations: List of conversation message lists, where each conversation\n",
    "                      is a list of dicts with 'role' and 'content' keys.\n",
    "                      Expected format: [[{'role': 'user', 'content': '...'}, \n",
    "                                        {'role': 'assistant', 'content': '...'}], ...]\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts in format: [{\"content\": {\"request\": \"...\", \"response\": \"...\"}}, ...]\n",
    "    \"\"\"\n",
    "    converted = []\n",
    "    \n",
    "    for conv in conversations:\n",
    "        user_message = None\n",
    "        assistant_message = None\n",
    "        \n",
    "        # Extract user and assistant messages\n",
    "        for msg in conv:\n",
    "            if msg['role'] == 'user':\n",
    "                user_message = msg['content']\n",
    "            elif msg['role'] == 'assistant':\n",
    "                assistant_message = msg['content']\n",
    "        \n",
    "        # Skip if either message is missing\n",
    "        if user_message is None or assistant_message is None:\n",
    "            continue\n",
    "        \n",
    "        # Create the new format\n",
    "        converted.append({\n",
    "            \"content\": {\n",
    "                \"request\": user_message,\n",
    "                \"response\": assistant_message\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return converted\n",
    "\n",
    "# Example usage:\n",
    "converted_test = convert_conversations_to_content_format(test_conversations)\n",
    "print(converted_test[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd06409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_converted_to_jsonl(converted_data, output_path):\n",
    "    \"\"\"\n",
    "    Save converted conversations to a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        converted_data: List of dicts in format [{\"content\": {\"request\": \"...\", \"response\": \"...\"}}, ...]\n",
    "        output_path: Path to save the output JSONL file (str or Path)\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Ensure output_path is a Path object\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    # Create parent directories if they don't exist\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Write to JSONL file\n",
    "    print(f\"Saving {len(converted_data)} converted conversations to {output_path}...\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in converted_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"✓ Successfully saved {len(converted_data)} conversations to {output_path}\")\n",
    "    print(f\"  File size: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Example usage:\n",
    "# converted_test = convert_conversations_to_content_format(test_conversations)\n",
    "# save_converted_to_jsonl(converted_test, \"/path/to/output.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d518c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING TRAIN DATASET\n",
      "================================================================================\n",
      "Creating instruct format for 10000 conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 316055.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 10000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct.jsonl...\n",
      "✓ Saved 10000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct.jsonl\n",
      "\n",
      "================================================================================\n",
      "CREATING VALIDATION DATASET\n",
      "================================================================================\n",
      "Creating instruct format for 1000 conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 374792.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct.jsonl...\n",
      "✓ Saved 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct.jsonl\n",
      "\n",
      "================================================================================\n",
      "CREATING VALIDATION DATASET\n",
      "================================================================================\n",
      "Creating instruct format for 1000 conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 321846.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_instruct.jsonl...\n",
      "✓ Saved 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_instruct.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Create Qwen4b Instruct datasets with judge evaluation feedback\n",
    "\n",
    "# System instruction for Qwen4b instruct model\n",
    "SYSTEM_INSTRUCTION = \"\"\"You are an expert academic abstract writer. Your task is to create a high-quality abstract for an arXiv paper based on the paper content and judge evaluation feedback.\n",
    "\n",
    "The judge evaluates abstracts based on five dimensions:\n",
    "1. Faithfulness: The abstract must accurately reflect the paper's content without hallucination\n",
    "2. Coverage: The abstract must include the essential aspects (main problem, approach, and key results)\n",
    "3. Clarity: The abstract must be understandable and readable\n",
    "4. Conciseness: The abstract must be focused and not verbose\n",
    "5. Coherence: The abstract must be logically structured and flow naturally\n",
    "\n",
    "When creating the abstract:\n",
    "- Read the paper content carefully\n",
    "- Pay attention to the judge's feedback on what makes a good abstract\n",
    "- Ensure your abstract meets all five evaluation criteria\n",
    "- Write a concise, clear, and coherent summary that accurately covers the paper's main contributions\n",
    "- Focus on the main problem, approach, and key results\n",
    "\n",
    "Your response should be the abstract only, without any additional commentary or explanation.\"\"\"\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_instruct_dataset(conversations, output_path, add_abstract=True):\n",
    "    \"\"\"\n",
    "    Create Qwen4b instruct dataset with judge evaluation criteria.\n",
    "    \n",
    "    Args:\n",
    "        conversations: List of conversation message lists\n",
    "        output_path: Path to save the output JSONL file\n",
    "    \"\"\"\n",
    "    # Create instruct format\n",
    "    instruct_conversations = []\n",
    "    \n",
    "    print(f\"Creating instruct format for {len(conversations)} conversations...\")\n",
    "    for conv in tqdm(conversations):\n",
    "        # Extract paper content (user message) and abstract (assistant message)\n",
    "        paper_content = None\n",
    "        abstract = None\n",
    "        \n",
    "        for msg in conv:\n",
    "            if msg['role'] == 'user':\n",
    "                paper_content = msg['content']\n",
    "            elif msg['role'] == 'assistant':\n",
    "                abstract = msg['content']\n",
    "        \n",
    "        if paper_content is None or abstract is None:\n",
    "            continue\n",
    "        \n",
    "        # Create user message with paper content and instructions about judge expectations\n",
    "        user_content = f\"\"\"Paper Content:\n",
    "{paper_content}\n",
    "\n",
    "---\n",
    "\n",
    "Create a high-quality abstract for this paper that meets all five evaluation criteria:\n",
    "1. Faithfulness: Accurately reflect the paper's content without hallucination\n",
    "2. Coverage: Include the essential aspects (main problem, approach, and key results)\n",
    "3. Clarity: Be understandable and readable\n",
    "4. Conciseness: Be focused and not verbose\n",
    "5. Coherence: Be logically structured and flow naturally\"\"\"\n",
    "        \n",
    "        # Create Qwen4b instruct format\n",
    "        new_conv = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": SYSTEM_INSTRUCTION\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_content\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        if add_abstract:\n",
    "            new_conv['messages'].append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": abstract\n",
    "            })\n",
    "        \n",
    "        # Append the conversation to the list\n",
    "        instruct_conversations.append(new_conv)\n",
    "    \n",
    "    # Save to JSONL\n",
    "    print(f\"\\nSaving {len(instruct_conversations)} conversations to {output_path}...\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for conv in instruct_conversations:\n",
    "            f.write(json.dumps(conv, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"✓ Saved {len(instruct_conversations)} conversations to {output_path}\")\n",
    "    \n",
    "    return instruct_conversations\n",
    "\n",
    "# Create train dataset\n",
    "train_instruct_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct.jsonl\"\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING TRAIN DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "train_instruct_conversations = create_instruct_dataset(\n",
    "    train_conversations,\n",
    "    train_instruct_path,\n",
    "    add_abstract=True\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "val_instruct_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct.jsonl\"\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VALIDATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "val_instruct_conversations = create_instruct_dataset(\n",
    "    val_conversations,\n",
    "    val_instruct_path,\n",
    "    add_abstract=True\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "test_instruct_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_instruct.jsonl\"\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VALIDATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "test_instruct_conversations = create_instruct_dataset(\n",
    "    test_conversations,\n",
    "    test_instruct_path,\n",
    "    add_abstract=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c6668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
