{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd621904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the arxiv-summarization dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f00d6",
   "metadata": {},
   "source": [
    "# Read hg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eec1c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "Dataset info:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 203037\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6436\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6440\n",
      "    })\n",
      "})\n",
      "\n",
      "train split:\n",
      "  Number of examples: 203037\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when comp...\n",
      "    abstract: additive models play an important role in semiparametric statistics . \n",
      " this paper gives learning rates for regularized kernel based methods for additive models . \n",
      " these learning rates compare favour...\n",
      "\n",
      "validation split:\n",
      "  Number of examples: 6436\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: the interest in anchoring phenomena and phenomena in confined nematic liquid crystals has largely been driven by their potential use in liquid crystal display devices . \n",
      " the twisted nematic liquid cr...\n",
      "    abstract: we study the phase behavior of a nematic liquid crystal confined between a flat substrate with strong anchoring and a patterned substrate whose structure and local anchoring strength we vary . by firs...\n",
      "\n",
      "test split:\n",
      "  Number of examples: 6440\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: for about 20 years the problem of properties of short - term changes of solar activity has been considered extensively . \n",
      " many investigators studied the short - term periodicities of the various indi...\n",
      "    abstract: the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \n",
      " the correlative analysis indicates negative correlation for the ...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "\n",
    "# Explore the dataset structure\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"\\nDataset info:\")\n",
    "print(dataset)\n",
    "\n",
    "# Check the structure of a sample from each split\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"\\n{split_name} split:\")\n",
    "    print(f\"  Number of examples: {len(dataset[split_name])}\")\n",
    "    if len(dataset[split_name]) > 0:\n",
    "        print(f\"  Features: {dataset[split_name].features}\")\n",
    "        print(f\"  First example keys: {list(dataset[split_name][0].keys())}\")\n",
    "        print(f\"  First example:\")\n",
    "        for key, value in dataset[split_name][0].items():\n",
    "            if isinstance(value, str) and len(value) > 200:\n",
    "                print(f\"    {key}: {value[:200]}...\")\n",
    "            else:\n",
    "                print(f\"    {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5a908",
   "metadata": {},
   "source": [
    "## write train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create oumi dataset for the entire training set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"arxiv_summarization_train_oumi.jsonl\"\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "print(f\"Converting {len(train_dataset)} training examples to oumi format...\")\n",
    "print(f\"Output file: {output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "# This avoids the overhead of creating Conversation/Message objects\n",
    "conversations_written = 0\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(train_dataset, desc=\"Processing\", total=len(train_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict (faster than creating objects)\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9d9ff",
   "metadata": {},
   "source": [
    "# Write validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80847c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 6436 validation examples to oumi format...\n",
      "Output file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation: 100%|██████████| 6436/6436 [00:00<00:00, 9249.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully created oumi dataset with 6436 conversations\n",
      "  File: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n",
      "  File size: 210.91 MB\n"
     ]
    }
   ],
   "source": [
    "# Create oumi dataset for validation set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_output_path = output_dir / \"arxiv_summarization_validation_oumi.jsonl\"\n",
    "\n",
    "val_dataset = dataset['validation']\n",
    "print(f\"Converting {len(val_dataset)} validation examples to oumi format...\")\n",
    "print(f\"Output file: {val_output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "conversations_written = 0\n",
    "with open(val_output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(val_dataset, desc=\"Processing validation\", total=len(val_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {val_output_path}\")\n",
    "print(f\"  File size: {val_output_path.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d08473",
   "metadata": {},
   "source": [
    "## write test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08faf176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 6440 test examples to oumi format...\n",
      "Output file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 6440/6440 [00:00<00:00, 9217.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully created oumi dataset with 6440 conversations\n",
      "  File: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n",
      "  File size: 211.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Create oumi dataset for test set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_output_path = output_dir / \"arxiv_summarization_test_oumi.jsonl\"\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "print(f\"Converting {len(test_dataset)} test examples to oumi format...\")\n",
    "print(f\"Output file: {test_output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "conversations_written = 0\n",
    "with open(test_output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(test_dataset, desc=\"Processing test\", total=len(test_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {test_output_path}\")\n",
    "print(f\"  File size: {test_output_path.stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec3f88",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a190e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "# Import oumi DatasetAnalyzer to get token length for Qwen3-8B\n",
    "from oumi.core.analyze import DatasetAnalyzer\n",
    "from oumi.core.configs import AnalyzeConfig, DatasetSource, SampleAnalyzerParams\n",
    "from oumi.datasets import TextSftJsonLinesDataset\n",
    "from oumi.core.types.conversation import Conversation, Message, Role\n",
    "import json\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc22e01",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4efb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the full training dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure output_path is defined (in case cell 3 wasn't run)\n",
    "if 'output_path' not in locals():\n",
    "    output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "    output_path = output_dir / \"arxiv_summarization_train_oumi.jsonl\"\n",
    "\n",
    "# Check if file exists\n",
    "if not Path(output_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(output_path))\n",
    "\n",
    "print(f\"Dataset loaded: {len(oumi_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_train\",\n",
    "    sample_count=20000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer (this may take a while for the full dataset)...\")\n",
    "analyzer = DatasetAnalyzer(config, dataset=oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    analyzer.analyze_dataset()\n",
    "    print(\"✓ Analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "        # The analysis should still have completed - the error is only in summary generation\n",
    "        # Try to access the results anyway\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results (should be available even if summary generation failed)\n",
    "analysis_df = analyzer.analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "filtered_dataset = analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)} conversations\")\n",
    "\n",
    "# Take first 10k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_train_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(10000, len(filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0297b",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5116173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation oumi dataset and running analysis...\n",
      "Dataset file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n",
      "[2025-11-12 16:04:29,196][oumi][rank0][pid:33744][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "Validation dataset loaded: 6436 conversations\n",
      "Running DatasetAnalyzer on validation set...\n",
      "[2025-11-12 16:04:29,651][oumi][rank0][pid:33744][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'Qwen/Qwen3-8B'.\n",
      "[2025-11-12 16:04:29,652][oumi.utils.analysis_utils][rank0][pid:33744][MainThread][INFO]][analysis_utils.py:57] Built tokenizer for model: Qwen/Qwen3-8B\n",
      "[2025-11-12 16:04:29,653][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:142] Using provided dataset 'arxiv_summarization_validation' with 6436 conversations\n",
      "[2025-11-12 16:04:29,653][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:275] Initialized sample analyzer: length\n",
      "[2025-11-12 16:04:29,654][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:301] Starting analysis of dataset: arxiv_summarization_validation\n",
      "[2025-11-12 16:04:29,655][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:302] Using 1 sample analyzers: ['length']\n",
      "[2025-11-12 16:04:29,655][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:325] Analyzing 2000 of 6436 conversations\n",
      "[2025-11-12 16:04:29,656][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:374] Converting conversation dataset with 6436 items\n",
      "[2025-11-12 16:04:29,656][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:381] Limiting analysis to first 2000 items (dataset has 6436 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting arxiv_summarization_validation to DataFrames: 100%|██████████| 2000/2000 [00:00<00:00, 4276.16item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:04:30,184][oumi][rank0][pid:33744][MainThread][WARNING]][dataframe_analyzer.py:101] Analyzer length failed: No text fields found in the DataFrame for length analysis. Please ensure your schema specifies columns withcontent_type='text'and that those columns exist in the DataFrame.\n",
      "✓ Validation analysis completed successfully\n",
      "\n",
      "✓ Validation analysis complete!\n",
      "  Total messages analyzed: 4000\n",
      "\n",
      "================================================================================\n",
      "Validation Set Summary Statistics by Role:\n",
      "================================================================================\n",
      "\n",
      "USER messages (2000 total):\n",
      "  Token count - Mean: 8948.6, Median: 7461.0, Min: 245, Max: 55237\n",
      "  Character count - Mean: 33872.9, Median: 29023.5\n",
      "  Word count - Mean: 6035.0, Median: 5166.0\n",
      "\n",
      "ASSISTANT messages (2000 total):\n",
      "  Token count - Mean: 199.7, Median: 192.0, Min: 52, Max: 697\n",
      "  Character count - Mean: 958.7, Median: 937.0\n",
      "  Word count - Mean: 161.4, Median: 157.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze the validation dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Validation dataset path\n",
    "val_output_path = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\")\n",
    "\n",
    "# Check if file exists\n",
    "if not val_output_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {val_output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading validation oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {val_output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "val_oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(val_output_path))\n",
    "\n",
    "print(f\"Validation dataset loaded: {len(val_oumi_dataset)} conversations\")\n",
    "\n",
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "val_config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_validation\",\n",
    "    sample_count=2000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer on validation set...\")\n",
    "val_analyzer = DatasetAnalyzer(val_config, dataset=val_oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    val_analyzer.analyze_dataset()\n",
    "    print(\"✓ Validation analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results\n",
    "val_analysis_df = val_analyzer.analysis_df\n",
    "\n",
    "print(f\"\\n✓ Validation analysis complete!\")\n",
    "print(f\"  Total messages analyzed: {len(val_analysis_df)}\")\n",
    "\n",
    "# Find columns with metrics\n",
    "token_cols = [col for col in val_analysis_df.columns if 'token_count' in col.lower()]\n",
    "char_cols = [col for col in val_analysis_df.columns if 'char_count' in col.lower()]\n",
    "word_cols = [col for col in val_analysis_df.columns if 'word_count' in col.lower()]\n",
    "\n",
    "# Display summary statistics by role\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Validation Set Summary Statistics by Role:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for role in ['user', 'assistant']:\n",
    "    role_rows = val_analysis_df[val_analysis_df['role'] == role]\n",
    "    if len(role_rows) > 0:\n",
    "        print(f\"\\n{role.upper()} messages ({len(role_rows)} total):\")\n",
    "        if token_cols:\n",
    "            token_col = token_cols[0]\n",
    "            print(f\"  Token count - Mean: {role_rows[token_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[token_col].median():.1f}, \"\n",
    "                  f\"Min: {role_rows[token_col].min()}, \"\n",
    "                  f\"Max: {role_rows[token_col].max()}\")\n",
    "        if char_cols:\n",
    "            char_col = char_cols[0]\n",
    "            print(f\"  Character count - Mean: {role_rows[char_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[char_col].median():.1f}\")\n",
    "        if word_cols:\n",
    "            word_col = word_cols[0]\n",
    "            print(f\"  Word count - Mean: {role_rows[word_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[word_col].median():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c24986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:07:32,031][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:435] Query 'text_content_token_count < 7500 & role == 'user'' returned 1008 rows\n",
      "[2025-11-12 16:07:32,033][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:580] Filtered dataset: 1008 conversations out of 6436 total\n",
      "Filtered dataset size: 1008 conversations\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "val_filtered_dataset = val_analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "print(f\"Filtered dataset size: {len(val_filtered_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e012a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving first 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_filtered_10k.jsonl...\n",
      "✓ Successfully saved 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_filtered_10k.jsonl\n",
      "  File size: 18.96 MB\n"
     ]
    }
   ],
   "source": [
    "# Take first 1k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_val_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(1000, len(val_filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = val_filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15484a6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8587a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test oumi dataset and running analysis...\n",
      "Dataset file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n",
      "[2025-11-12 16:08:47,406][oumi][rank0][pid:33744][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "Test dataset loaded: 6440 conversations\n",
      "Running DatasetAnalyzer on test set...\n",
      "[2025-11-12 16:08:47,803][oumi][rank0][pid:33744][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'Qwen/Qwen3-8B'.\n",
      "[2025-11-12 16:08:47,803][oumi.utils.analysis_utils][rank0][pid:33744][MainThread][INFO]][analysis_utils.py:57] Built tokenizer for model: Qwen/Qwen3-8B\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:142] Using provided dataset 'arxiv_summarization_test' with 6440 conversations\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:275] Initialized sample analyzer: length\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:301] Starting analysis of dataset: arxiv_summarization_test\n",
      "[2025-11-12 16:08:47,805][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:302] Using 1 sample analyzers: ['length']\n",
      "[2025-11-12 16:08:47,805][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:325] Analyzing 2000 of 6440 conversations\n",
      "[2025-11-12 16:08:47,806][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:374] Converting conversation dataset with 6440 items\n",
      "[2025-11-12 16:08:47,807][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:381] Limiting analysis to first 2000 items (dataset has 6440 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting arxiv_summarization_test to DataFrames: 100%|██████████| 2000/2000 [00:00<00:00, 6007.58item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:08:48,348][oumi][rank0][pid:33744][MainThread][WARNING]][dataframe_analyzer.py:101] Analyzer length failed: No text fields found in the DataFrame for length analysis. Please ensure your schema specifies columns withcontent_type='text'and that those columns exist in the DataFrame.\n",
      "✓ Test analysis completed successfully\n",
      "\n",
      "✓ Test analysis complete!\n",
      "  Total messages analyzed: 4000\n",
      "\n",
      "================================================================================\n",
      "Test Set Summary Statistics by Role:\n",
      "================================================================================\n",
      "\n",
      "USER messages (2000 total):\n",
      "  Token count - Mean: 8772.7, Median: 7324.0, Min: 282, Max: 67349\n",
      "  Character count - Mean: 33309.4, Median: 28449.0\n",
      "  Word count - Mean: 5937.6, Median: 5080.0\n",
      "\n",
      "ASSISTANT messages (2000 total):\n",
      "  Token count - Mean: 203.5, Median: 198.0, Min: 54, Max: 608\n",
      "  Character count - Mean: 973.0, Median: 962.5\n",
      "  Word count - Mean: 164.2, Median: 161.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze the test dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Test dataset path\n",
    "test_output_path = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\")\n",
    "\n",
    "# Check if file exists\n",
    "if not test_output_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {test_output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading test oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {test_output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "test_oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(test_output_path))\n",
    "\n",
    "print(f\"Test dataset loaded: {len(test_oumi_dataset)} conversations\")\n",
    "\n",
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "test_config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_test\",\n",
    "    sample_count=2000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer on test set...\")\n",
    "test_analyzer = DatasetAnalyzer(test_config, dataset=test_oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    test_analyzer.analyze_dataset()\n",
    "    print(\"✓ Test analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results\n",
    "test_analysis_df = test_analyzer.analysis_df\n",
    "\n",
    "print(f\"\\n✓ Test analysis complete!\")\n",
    "print(f\"  Total messages analyzed: {len(test_analysis_df)}\")\n",
    "\n",
    "# Find columns with metrics\n",
    "token_cols = [col for col in test_analysis_df.columns if 'token_count' in col.lower()]\n",
    "char_cols = [col for col in test_analysis_df.columns if 'char_count' in col.lower()]\n",
    "word_cols = [col for col in test_analysis_df.columns if 'word_count' in col.lower()]\n",
    "\n",
    "# Display summary statistics by role\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Test Set Summary Statistics by Role:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for role in ['user', 'assistant']:\n",
    "    role_rows = test_analysis_df[test_analysis_df['role'] == role]\n",
    "    if len(role_rows) > 0:\n",
    "        print(f\"\\n{role.upper()} messages ({len(role_rows)} total):\")\n",
    "        if token_cols:\n",
    "            token_col = token_cols[0]\n",
    "            print(f\"  Token count - Mean: {role_rows[token_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[token_col].median():.1f}, \"\n",
    "                  f\"Min: {role_rows[token_col].min()}, \"\n",
    "                  f\"Max: {role_rows[token_col].max()}\")\n",
    "        if char_cols:\n",
    "            char_col = char_cols[0]\n",
    "            print(f\"  Character count - Mean: {role_rows[char_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[char_col].median():.1f}\")\n",
    "        if word_cols:\n",
    "            word_col = word_cols[0]\n",
    "            print(f\"  Word count - Mean: {role_rows[word_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[word_col].median():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f27b39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:09:12,514][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:435] Query 'text_content_token_count < 7500 & role == 'user'' returned 1026 rows\n",
      "[2025-11-12 16:09:12,516][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:580] Filtered dataset: 1026 conversations out of 6440 total\n",
      "Filtered dataset size: 1026 conversations\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "test_filtered_dataset = test_analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "print(f\"Filtered dataset size: {len(test_filtered_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "befd5f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving first 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_filtered_10k.jsonl...\n",
      "✓ Successfully saved 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_filtered_10k.jsonl\n",
      "  File size: 19.09 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Take first 1k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_test_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(1000, len(test_filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = test_filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eecf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
