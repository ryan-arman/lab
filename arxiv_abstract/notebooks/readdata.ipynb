{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd621904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the arxiv-summarization dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f00d6",
   "metadata": {},
   "source": [
    "# Read hg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eec1c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "Dataset info:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 203037\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6436\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6440\n",
      "    })\n",
      "})\n",
      "\n",
      "train split:\n",
      "  Number of examples: 203037\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when comp...\n",
      "    abstract: additive models play an important role in semiparametric statistics . \n",
      " this paper gives learning rates for regularized kernel based methods for additive models . \n",
      " these learning rates compare favour...\n",
      "\n",
      "validation split:\n",
      "  Number of examples: 6436\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: the interest in anchoring phenomena and phenomena in confined nematic liquid crystals has largely been driven by their potential use in liquid crystal display devices . \n",
      " the twisted nematic liquid cr...\n",
      "    abstract: we study the phase behavior of a nematic liquid crystal confined between a flat substrate with strong anchoring and a patterned substrate whose structure and local anchoring strength we vary . by firs...\n",
      "\n",
      "test split:\n",
      "  Number of examples: 6440\n",
      "  Features: {'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n",
      "  First example keys: ['article', 'abstract']\n",
      "  First example:\n",
      "    article: for about 20 years the problem of properties of short - term changes of solar activity has been considered extensively . \n",
      " many investigators studied the short - term periodicities of the various indi...\n",
      "    abstract: the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \n",
      " the correlative analysis indicates negative correlation for the ...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "\n",
    "# Explore the dataset structure\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"\\nDataset info:\")\n",
    "print(dataset)\n",
    "\n",
    "# Check the structure of a sample from each split\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"\\n{split_name} split:\")\n",
    "    print(f\"  Number of examples: {len(dataset[split_name])}\")\n",
    "    if len(dataset[split_name]) > 0:\n",
    "        print(f\"  Features: {dataset[split_name].features}\")\n",
    "        print(f\"  First example keys: {list(dataset[split_name][0].keys())}\")\n",
    "        print(f\"  First example:\")\n",
    "        for key, value in dataset[split_name][0].items():\n",
    "            if isinstance(value, str) and len(value) > 200:\n",
    "                print(f\"    {key}: {value[:200]}...\")\n",
    "            else:\n",
    "                print(f\"    {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5a908",
   "metadata": {},
   "source": [
    "## write train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create oumi dataset for the entire training set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"arxiv_summarization_train_oumi.jsonl\"\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "print(f\"Converting {len(train_dataset)} training examples to oumi format...\")\n",
    "print(f\"Output file: {output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "# This avoids the overhead of creating Conversation/Message objects\n",
    "conversations_written = 0\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(train_dataset, desc=\"Processing\", total=len(train_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict (faster than creating objects)\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9d9ff",
   "metadata": {},
   "source": [
    "# Write validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80847c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 6436 validation examples to oumi format...\n",
      "Output file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation: 100%|██████████| 6436/6436 [00:00<00:00, 9249.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully created oumi dataset with 6436 conversations\n",
      "  File: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n",
      "  File size: 210.91 MB\n"
     ]
    }
   ],
   "source": [
    "# Create oumi dataset for validation set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_output_path = output_dir / \"arxiv_summarization_validation_oumi.jsonl\"\n",
    "\n",
    "val_dataset = dataset['validation']\n",
    "print(f\"Converting {len(val_dataset)} validation examples to oumi format...\")\n",
    "print(f\"Output file: {val_output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "conversations_written = 0\n",
    "with open(val_output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(val_dataset, desc=\"Processing validation\", total=len(val_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {val_output_path}\")\n",
    "print(f\"  File size: {val_output_path.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d08473",
   "metadata": {},
   "source": [
    "## write test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08faf176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 6440 test examples to oumi format...\n",
      "Output file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 6440/6440 [00:00<00:00, 9217.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully created oumi dataset with 6440 conversations\n",
      "  File: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n",
      "  File size: 211.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Create oumi dataset for test set\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output path for the oumi dataset\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_output_path = output_dir / \"arxiv_summarization_test_oumi.jsonl\"\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "print(f\"Converting {len(test_dataset)} test examples to oumi format...\")\n",
    "print(f\"Output file: {test_output_path}\")\n",
    "\n",
    "# Optimized: Iterate directly over the dataset and create dict directly\n",
    "conversations_written = 0\n",
    "with open(test_output_path, 'w', encoding='utf-8') as f:\n",
    "    for example in tqdm(test_dataset, desc=\"Processing test\", total=len(test_dataset)):\n",
    "        article = example['article']\n",
    "        abstract = example['abstract']\n",
    "        \n",
    "        # Create oumi conversation format directly as dict\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "            {\"role\": \"assistant\", \"content\": abstract}\n",
    "        ]\n",
    "        \n",
    "        # Write to JSONL file (one JSON object per line)\n",
    "        json.dump({\"messages\": messages}, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "        conversations_written += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully created oumi dataset with {conversations_written} conversations\")\n",
    "print(f\"  File: {test_output_path}\")\n",
    "print(f\"  File size: {test_output_path.stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec3f88",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a190e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "# Import oumi DatasetAnalyzer to get token length for Qwen3-8B\n",
    "from oumi.core.analyze import DatasetAnalyzer\n",
    "from oumi.core.configs import AnalyzeConfig, DatasetSource, SampleAnalyzerParams\n",
    "from oumi.datasets import TextSftJsonLinesDataset\n",
    "from oumi.core.types.conversation import Conversation, Message, Role\n",
    "import json\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc22e01",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f4efb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading oumi dataset and running analysis...\n",
      "Dataset file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_oumi.jsonl\n",
      "[2025-11-12 16:40:12,131][oumi][rank0][pid:33744][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "Dataset loaded: 203037 conversations\n"
     ]
    }
   ],
   "source": [
    "# Analyze the full training dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure output_path is defined (in case cell 3 wasn't run)\n",
    "if 'output_path' not in locals():\n",
    "    output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "    output_path = output_dir / \"arxiv_summarization_train_oumi.jsonl\"\n",
    "\n",
    "# Check if file exists\n",
    "if not Path(output_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "train_oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(output_path))\n",
    "\n",
    "print(f\"Dataset loaded: {len(train_oumi_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6306702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DatasetAnalyzer (this may take a while for the full dataset)...\n",
      "[2025-11-12 16:40:13,278][oumi][rank0][pid:33744][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'Qwen/Qwen3-8B'.\n",
      "[2025-11-12 16:40:13,279][oumi.utils.analysis_utils][rank0][pid:33744][MainThread][INFO]][analysis_utils.py:57] Built tokenizer for model: Qwen/Qwen3-8B\n",
      "[2025-11-12 16:40:13,280][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:142] Using provided dataset 'arxiv_summarization_train' with 203037 conversations\n",
      "[2025-11-12 16:40:13,280][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:275] Initialized sample analyzer: length\n",
      "[2025-11-12 16:40:13,280][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:301] Starting analysis of dataset: arxiv_summarization_train\n",
      "[2025-11-12 16:40:13,282][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:302] Using 1 sample analyzers: ['length']\n",
      "[2025-11-12 16:40:13,283][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:325] Analyzing 20000 of 203037 conversations\n",
      "[2025-11-12 16:40:13,283][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:374] Converting conversation dataset with 203037 items\n",
      "[2025-11-12 16:40:13,283][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:381] Limiting analysis to first 20000 items (dataset has 203037 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting arxiv_summarization_train to DataFrames: 100%|██████████| 20000/20000 [00:04<00:00, 4640.84item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:40:18,566][oumi][rank0][pid:33744][MainThread][WARNING]][dataframe_analyzer.py:101] Analyzer length failed: No text fields found in the DataFrame for length analysis. Please ensure your schema specifies columns withcontent_type='text'and that those columns exist in the DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (136905 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Analysis completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_train\",\n",
    "    sample_count=20000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer (this may take a while for the full dataset)...\")\n",
    "analyzer = DatasetAnalyzer(config, dataset=train_oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    analyzer.analyze_dataset()\n",
    "    print(\"✓ Analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "        # The analysis should still have completed - the error is only in summary generation\n",
    "        # Try to access the results anyway\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results (should be available even if summary generation failed)\n",
    "analysis_df = analyzer.analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a14466f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "839.5079106846358"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analysis_df.query(\"role == 'assistant'\")['text_content_token_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e38be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:44:34,727][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:435] Query 'text_content_token_count < 7500 & role == 'user'' returned 10402 rows\n",
      "[2025-11-12 16:44:34,732][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:580] Filtered dataset: 10402 conversations out of 203037 total\n",
      "Filtered dataset size: 10402 conversations\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "train_filtered_dataset = analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "\n",
    "print(f\"Filtered dataset size: {len(train_filtered_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take first 10k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_train_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(10000, len(train_filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = train_filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0297b",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5116173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation oumi dataset and running analysis...\n",
      "Dataset file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\n",
      "[2025-11-12 16:04:29,196][oumi][rank0][pid:33744][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "Validation dataset loaded: 6436 conversations\n",
      "Running DatasetAnalyzer on validation set...\n",
      "[2025-11-12 16:04:29,651][oumi][rank0][pid:33744][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'Qwen/Qwen3-8B'.\n",
      "[2025-11-12 16:04:29,652][oumi.utils.analysis_utils][rank0][pid:33744][MainThread][INFO]][analysis_utils.py:57] Built tokenizer for model: Qwen/Qwen3-8B\n",
      "[2025-11-12 16:04:29,653][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:142] Using provided dataset 'arxiv_summarization_validation' with 6436 conversations\n",
      "[2025-11-12 16:04:29,653][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:275] Initialized sample analyzer: length\n",
      "[2025-11-12 16:04:29,654][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:301] Starting analysis of dataset: arxiv_summarization_validation\n",
      "[2025-11-12 16:04:29,655][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:302] Using 1 sample analyzers: ['length']\n",
      "[2025-11-12 16:04:29,655][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:325] Analyzing 2000 of 6436 conversations\n",
      "[2025-11-12 16:04:29,656][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:374] Converting conversation dataset with 6436 items\n",
      "[2025-11-12 16:04:29,656][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:381] Limiting analysis to first 2000 items (dataset has 6436 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting arxiv_summarization_validation to DataFrames: 100%|██████████| 2000/2000 [00:00<00:00, 4276.16item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:04:30,184][oumi][rank0][pid:33744][MainThread][WARNING]][dataframe_analyzer.py:101] Analyzer length failed: No text fields found in the DataFrame for length analysis. Please ensure your schema specifies columns withcontent_type='text'and that those columns exist in the DataFrame.\n",
      "✓ Validation analysis completed successfully\n",
      "\n",
      "✓ Validation analysis complete!\n",
      "  Total messages analyzed: 4000\n",
      "\n",
      "================================================================================\n",
      "Validation Set Summary Statistics by Role:\n",
      "================================================================================\n",
      "\n",
      "USER messages (2000 total):\n",
      "  Token count - Mean: 8948.6, Median: 7461.0, Min: 245, Max: 55237\n",
      "  Character count - Mean: 33872.9, Median: 29023.5\n",
      "  Word count - Mean: 6035.0, Median: 5166.0\n",
      "\n",
      "ASSISTANT messages (2000 total):\n",
      "  Token count - Mean: 199.7, Median: 192.0, Min: 52, Max: 697\n",
      "  Character count - Mean: 958.7, Median: 937.0\n",
      "  Word count - Mean: 161.4, Median: 157.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze the validation dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Validation dataset path\n",
    "val_output_path = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_validation_oumi.jsonl\")\n",
    "\n",
    "# Check if file exists\n",
    "if not val_output_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {val_output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading validation oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {val_output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "val_oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(val_output_path))\n",
    "\n",
    "print(f\"Validation dataset loaded: {len(val_oumi_dataset)} conversations\")\n",
    "\n",
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "val_config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_validation\",\n",
    "    sample_count=2000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer on validation set...\")\n",
    "val_analyzer = DatasetAnalyzer(val_config, dataset=val_oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    val_analyzer.analyze_dataset()\n",
    "    print(\"✓ Validation analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results\n",
    "val_analysis_df = val_analyzer.analysis_df\n",
    "\n",
    "print(f\"\\n✓ Validation analysis complete!\")\n",
    "print(f\"  Total messages analyzed: {len(val_analysis_df)}\")\n",
    "\n",
    "# Find columns with metrics\n",
    "token_cols = [col for col in val_analysis_df.columns if 'token_count' in col.lower()]\n",
    "char_cols = [col for col in val_analysis_df.columns if 'char_count' in col.lower()]\n",
    "word_cols = [col for col in val_analysis_df.columns if 'word_count' in col.lower()]\n",
    "\n",
    "# Display summary statistics by role\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Validation Set Summary Statistics by Role:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for role in ['user', 'assistant']:\n",
    "    role_rows = val_analysis_df[val_analysis_df['role'] == role]\n",
    "    if len(role_rows) > 0:\n",
    "        print(f\"\\n{role.upper()} messages ({len(role_rows)} total):\")\n",
    "        if token_cols:\n",
    "            token_col = token_cols[0]\n",
    "            print(f\"  Token count - Mean: {role_rows[token_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[token_col].median():.1f}, \"\n",
    "                  f\"Min: {role_rows[token_col].min()}, \"\n",
    "                  f\"Max: {role_rows[token_col].max()}\")\n",
    "        if char_cols:\n",
    "            char_col = char_cols[0]\n",
    "            print(f\"  Character count - Mean: {role_rows[char_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[char_col].median():.1f}\")\n",
    "        if word_cols:\n",
    "            word_col = word_cols[0]\n",
    "            print(f\"  Word count - Mean: {role_rows[word_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[word_col].median():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c24986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:07:32,031][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:435] Query 'text_content_token_count < 7500 & role == 'user'' returned 1008 rows\n",
      "[2025-11-12 16:07:32,033][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:580] Filtered dataset: 1008 conversations out of 6436 total\n",
      "Filtered dataset size: 1008 conversations\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "val_filtered_dataset = val_analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "print(f\"Filtered dataset size: {len(val_filtered_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e012a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving first 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_filtered_10k.jsonl...\n",
      "✓ Successfully saved 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_filtered_10k.jsonl\n",
      "  File size: 18.96 MB\n"
     ]
    }
   ],
   "source": [
    "# Take first 1k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_val_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(1000, len(val_filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = val_filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15484a6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8587a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test oumi dataset and running analysis...\n",
      "Dataset file: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\n",
      "[2025-11-12 16:08:47,406][oumi][rank0][pid:33744][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "Test dataset loaded: 6440 conversations\n",
      "Running DatasetAnalyzer on test set...\n",
      "[2025-11-12 16:08:47,803][oumi][rank0][pid:33744][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'Qwen/Qwen3-8B'.\n",
      "[2025-11-12 16:08:47,803][oumi.utils.analysis_utils][rank0][pid:33744][MainThread][INFO]][analysis_utils.py:57] Built tokenizer for model: Qwen/Qwen3-8B\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:142] Using provided dataset 'arxiv_summarization_test' with 6440 conversations\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:275] Initialized sample analyzer: length\n",
      "[2025-11-12 16:08:47,804][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:301] Starting analysis of dataset: arxiv_summarization_test\n",
      "[2025-11-12 16:08:47,805][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:302] Using 1 sample analyzers: ['length']\n",
      "[2025-11-12 16:08:47,805][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:325] Analyzing 2000 of 6440 conversations\n",
      "[2025-11-12 16:08:47,806][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:374] Converting conversation dataset with 6440 items\n",
      "[2025-11-12 16:08:47,807][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:381] Limiting analysis to first 2000 items (dataset has 6440 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting arxiv_summarization_test to DataFrames: 100%|██████████| 2000/2000 [00:00<00:00, 6007.58item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:08:48,348][oumi][rank0][pid:33744][MainThread][WARNING]][dataframe_analyzer.py:101] Analyzer length failed: No text fields found in the DataFrame for length analysis. Please ensure your schema specifies columns withcontent_type='text'and that those columns exist in the DataFrame.\n",
      "✓ Test analysis completed successfully\n",
      "\n",
      "✓ Test analysis complete!\n",
      "  Total messages analyzed: 4000\n",
      "\n",
      "================================================================================\n",
      "Test Set Summary Statistics by Role:\n",
      "================================================================================\n",
      "\n",
      "USER messages (2000 total):\n",
      "  Token count - Mean: 8772.7, Median: 7324.0, Min: 282, Max: 67349\n",
      "  Character count - Mean: 33309.4, Median: 28449.0\n",
      "  Word count - Mean: 5937.6, Median: 5080.0\n",
      "\n",
      "ASSISTANT messages (2000 total):\n",
      "  Token count - Mean: 203.5, Median: 198.0, Min: 54, Max: 608\n",
      "  Character count - Mean: 973.0, Median: 962.5\n",
      "  Word count - Mean: 164.2, Median: 161.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze the test dataset using DatasetAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Test dataset path\n",
    "test_output_path = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_oumi.jsonl\")\n",
    "\n",
    "# Check if file exists\n",
    "if not test_output_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset file not found: {test_output_path}\\n\"\n",
    "        f\"Please run the previous cell to create the dataset first.\"\n",
    "    )\n",
    "\n",
    "print(\"Loading test oumi dataset and running analysis...\")\n",
    "print(f\"Dataset file: {test_output_path}\")\n",
    "\n",
    "# Load the oumi dataset\n",
    "test_oumi_dataset = TextSftJsonLinesDataset(dataset_path=str(test_output_path))\n",
    "\n",
    "print(f\"Test dataset loaded: {len(test_oumi_dataset)} conversations\")\n",
    "\n",
    "# Configure analyzer with Qwen3-8B tokenizer and length analyzer\n",
    "test_config = AnalyzeConfig(\n",
    "    dataset_source=DatasetSource.DIRECT,\n",
    "    dataset_name=\"arxiv_summarization_test\",\n",
    "    sample_count=2000,\n",
    "    analyzers=[\n",
    "        SampleAnalyzerParams(\n",
    "            id=\"length\",\n",
    "            params={\n",
    "                \"char_count\": True,\n",
    "                \"word_count\": True,\n",
    "                \"token_count\": True,\n",
    "                \"include_special_tokens\": True\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    tokenizer_config={\n",
    "        \"model_name\": \"Qwen/Qwen3-8B\",\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and run the analyzer\n",
    "print(\"Running DatasetAnalyzer on test set...\")\n",
    "test_analyzer = DatasetAnalyzer(test_config, dataset=test_oumi_dataset)\n",
    "\n",
    "# Run analysis - catch numpy compatibility error in summary generation\n",
    "try:\n",
    "    test_analyzer.analyze_dataset()\n",
    "    print(\"✓ Test analysis completed successfully\")\n",
    "except AttributeError as e:\n",
    "    if \"module 'numpy' has no attribute 'ma'\" in str(e):\n",
    "        print(\"⚠ Warning: Analysis completed but summary generation failed due to numpy compatibility issue.\")\n",
    "        print(\"  The analysis data is still available, but summary statistics may be incomplete.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Get the analysis results\n",
    "test_analysis_df = test_analyzer.analysis_df\n",
    "\n",
    "print(f\"\\n✓ Test analysis complete!\")\n",
    "print(f\"  Total messages analyzed: {len(test_analysis_df)}\")\n",
    "\n",
    "# Find columns with metrics\n",
    "token_cols = [col for col in test_analysis_df.columns if 'token_count' in col.lower()]\n",
    "char_cols = [col for col in test_analysis_df.columns if 'char_count' in col.lower()]\n",
    "word_cols = [col for col in test_analysis_df.columns if 'word_count' in col.lower()]\n",
    "\n",
    "# Display summary statistics by role\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Test Set Summary Statistics by Role:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for role in ['user', 'assistant']:\n",
    "    role_rows = test_analysis_df[test_analysis_df['role'] == role]\n",
    "    if len(role_rows) > 0:\n",
    "        print(f\"\\n{role.upper()} messages ({len(role_rows)} total):\")\n",
    "        if token_cols:\n",
    "            token_col = token_cols[0]\n",
    "            print(f\"  Token count - Mean: {role_rows[token_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[token_col].median():.1f}, \"\n",
    "                  f\"Min: {role_rows[token_col].min()}, \"\n",
    "                  f\"Max: {role_rows[token_col].max()}\")\n",
    "        if char_cols:\n",
    "            char_col = char_cols[0]\n",
    "            print(f\"  Character count - Mean: {role_rows[char_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[char_col].median():.1f}\")\n",
    "        if word_cols:\n",
    "            word_col = word_cols[0]\n",
    "            print(f\"  Word count - Mean: {role_rows[word_col].mean():.1f}, \"\n",
    "                  f\"Median: {role_rows[word_col].median():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f27b39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 16:09:12,514][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:435] Query 'text_content_token_count < 7500 & role == 'user'' returned 1026 rows\n",
      "[2025-11-12 16:09:12,516][oumi][rank0][pid:33744][MainThread][INFO]][dataset_analyzer.py:580] Filtered dataset: 1026 conversations out of 6440 total\n",
      "Filtered dataset size: 1026 conversations\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to conversations where user (article) token count < 7500\n",
    "test_filtered_dataset = test_analyzer.filter(\"text_content_token_count < 7500 & role == 'user'\")\n",
    "print(f\"Filtered dataset size: {len(test_filtered_dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "befd5f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving first 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_filtered_10k.jsonl...\n",
      "✓ Successfully saved 1000 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_filtered_10k.jsonl\n",
      "  File size: 19.09 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Take first 1k samples and save to JSONL using oumi's save_jsonlines utility\n",
    "from pathlib import Path\n",
    "from oumi.utils.io_utils import save_jsonlines\n",
    "\n",
    "output_dir = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"arxiv_summarization_test_filtered_10k.jsonl\"\n",
    "\n",
    "# Limit to first 10k conversations\n",
    "num_samples = min(1000, len(test_filtered_dataset))\n",
    "print(f\"Saving first {num_samples} conversations to {output_file}...\")\n",
    "\n",
    "# Extract conversations and convert to dict format\n",
    "# Use raw() method to get unconverted data to avoid tokenization\n",
    "conversations_data = []\n",
    "for i in range(num_samples):\n",
    "    # Access raw data (pandas Series) to avoid tokenization\n",
    "    raw_sample = test_filtered_dataset.raw(i)\n",
    "    \n",
    "    # The raw data is stored in _messages_column (pandas Series)\n",
    "    # Extract the conversation dict from the Series\n",
    "    if '_messages_column' in raw_sample:\n",
    "        conversation_dict = raw_sample['_messages_column']\n",
    "    else:\n",
    "        # Fallback: get the first value if it's a Series\n",
    "        conversation_dict = raw_sample.iloc[0] if hasattr(raw_sample, 'iloc') else raw_sample\n",
    "    \n",
    "    # conversation_dict should now be the original dict from the JSONL file\n",
    "    # Ensure it's in the right format\n",
    "    if isinstance(conversation_dict, dict):\n",
    "        conversations_data.append(conversation_dict)\n",
    "    else:\n",
    "        # If it's somehow not a dict, try to convert\n",
    "        if hasattr(conversation_dict, 'to_dict'):\n",
    "            conversations_data.append(conversation_dict.to_dict())\n",
    "        else:\n",
    "            conversations_data.append({\"messages\": getattr(conversation_dict, 'messages', [])})\n",
    "\n",
    "# Save using oumi's save_jsonlines utility\n",
    "save_jsonlines(output_file, conversations_data)\n",
    "\n",
    "print(f\"✓ Successfully saved {len(conversations_data)} conversations to {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eecf73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4d4c31",
   "metadata": {},
   "source": [
    "# System instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af02355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    evaluate_summary,\n",
    "    evaluate_summaries_batch,\n",
    "    display_text,\n",
    "    display_message,\n",
    "    load_conversations,\n",
    "    client,\n",
    "    JUDGE_SYSTEM_INSTRUCTION,\n",
    "    JUDGE_PROMPT_TEMPLATE_WITH_REQUEST_AND_RESPONSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d24b4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_filtered_10k.jsonl\"\n",
    "val_path =   \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_filtered_10k.jsonl\"\n",
    "test_path =  \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_filtered_10k.jsonl\"\n",
    "# train_distilled_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_abstract_train_gpt5mini_think2.jsonl\"\n",
    "train_conversations = load_conversations(train_path)\n",
    "val_conversations = load_conversations(val_path)\n",
    "test_conversations = load_conversations(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d518c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING TRAIN DATASET\n",
      "================================================================================\n",
      "Creating instruct format for 10000 conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 166602.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 0 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct.jsonl...\n",
      "✓ Saved 0 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct.jsonl\n",
      "\n",
      "================================================================================\n",
      "CREATING VALIDATION DATASET\n",
      "================================================================================\n",
      "Creating instruct format for 1000 conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 153468.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 0 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct.jsonl...\n",
      "✓ Saved 0 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct.jsonl\n",
      "\n",
      "================================================================================\n",
      "CREATING VALIDATION DATASET\n",
      "================================================================================\n",
      "Creating instruct format for 1000 conversations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 185293.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 0 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_instruct.jsonl...\n",
      "✓ Saved 0 conversations to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_instruct.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Create Qwen4b Instruct datasets with judge evaluation feedback\n",
    "\n",
    "# System instruction for Qwen4b instruct model\n",
    "SYSTEM_INSTRUCTION = \"\"\"You are an expert academic abstract writer. Your task is to create a high-quality abstract for an arXiv paper based on the paper content and judge evaluation feedback.\n",
    "\n",
    "The judge evaluates abstracts based on five dimensions:\n",
    "1. Faithfulness: The abstract must accurately reflect the paper's content without hallucination\n",
    "2. Coverage: The abstract must include the essential aspects (main problem, approach, and key results)\n",
    "3. Clarity: The abstract must be understandable and readable\n",
    "4. Conciseness: The abstract must be focused and not verbose\n",
    "5. Coherence: The abstract must be logically structured and flow naturally\n",
    "\n",
    "When creating the abstract:\n",
    "- Read the paper content carefully\n",
    "- Pay attention to the judge's feedback on what makes a good abstract\n",
    "- Ensure your abstract meets all five evaluation criteria\n",
    "- Write a concise, clear, and coherent summary that accurately covers the paper's main contributions\n",
    "- Focus on the main problem, approach, and key results\n",
    "\n",
    "Your response should be the abstract only, without any additional commentary or explanation.\"\"\"\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_instruct_dataset(conversations, output_path, add_abstract=True):\n",
    "    \"\"\"\n",
    "    Create Qwen4b instruct dataset with judge evaluation criteria.\n",
    "    \n",
    "    Args:\n",
    "        conversations: List of conversation message lists\n",
    "        output_path: Path to save the output JSONL file\n",
    "    \"\"\"\n",
    "    # Create instruct format\n",
    "    instruct_conversations = []\n",
    "    \n",
    "    print(f\"Creating instruct format for {len(conversations)} conversations...\")\n",
    "    for conv in tqdm(conversations):\n",
    "        # Extract paper content (user message) and abstract (assistant message)\n",
    "        paper_content = None\n",
    "        abstract = None\n",
    "        \n",
    "        for msg in conv:\n",
    "            if msg['role'] == 'user':\n",
    "                paper_content = msg['content']\n",
    "            elif msg['role'] == 'assistant':\n",
    "                abstract = msg['content']\n",
    "        \n",
    "        if paper_content is None or abstract is None:\n",
    "            continue\n",
    "        \n",
    "        # Create user message with paper content and instructions about judge expectations\n",
    "        user_content = f\"\"\"Paper Content:\n",
    "{paper_content}\n",
    "\n",
    "---\n",
    "\n",
    "Create a high-quality abstract for this paper that meets all five evaluation criteria:\n",
    "1. Faithfulness: Accurately reflect the paper's content without hallucination\n",
    "2. Coverage: Include the essential aspects (main problem, approach, and key results)\n",
    "3. Clarity: Be understandable and readable\n",
    "4. Conciseness: Be focused and not verbose\n",
    "5. Coherence: Be logically structured and flow naturally\"\"\"\n",
    "        \n",
    "        # Create Qwen4b instruct format\n",
    "        new_conv = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": SYSTEM_INSTRUCTION\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_content\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        if add_abstract:\n",
    "            new_conv['messages'].append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": abstract\n",
    "            })\n",
    "        \n",
    "        # Append the conversation to the list\n",
    "        instruct_conversations.append(new_conv)\n",
    "    \n",
    "    # Save to JSONL\n",
    "    print(f\"\\nSaving {len(instruct_conversations)} conversations to {output_path}...\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for conv in instruct_conversations:\n",
    "            f.write(json.dumps(conv, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"✓ Saved {len(instruct_conversations)} conversations to {output_path}\")\n",
    "    \n",
    "    return instruct_conversations\n",
    "\n",
    "# Create train dataset\n",
    "train_instruct_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct.jsonl\"\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING TRAIN DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "train_instruct_conversations = create_instruct_dataset(\n",
    "    train_conversations,\n",
    "    train_instruct_path,\n",
    "    add_abstract=True\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "val_instruct_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct.jsonl\"\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VALIDATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "val_instruct_conversations = create_instruct_dataset(\n",
    "    val_conversations,\n",
    "    val_instruct_path,\n",
    "    add_abstract=True\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "test_instruct_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_test_instruct.jsonl\"\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VALIDATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "test_instruct_conversations = create_instruct_dataset(\n",
    "    test_conversations,\n",
    "    test_instruct_path,\n",
    "    add_abstract=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c6668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
