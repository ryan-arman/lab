{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate High-Quality Abstracts and Rewritten Articles using GPT-5\n",
        "\n",
        "This notebook generates high-quality abstracts and rewritten articles for training and validation datasets using OpenAI's GPT-5 (or other high-quality models).\n",
        "\n",
        "The notebook will:\n",
        "1. Generate high-quality abstracts that meet all five evaluation criteria (Faithfulness, Coverage, Clarity, Conciseness, Coherence)\n",
        "2. Rewrite the article content to:\n",
        "   - Remove noise and placeholders (e.g., equation placeholders, figure references)\n",
        "   - Condense the content (focusing on introduction, conclusion, and discussion sections)\n",
        "   - Clean up the text for better readability\n",
        "\n",
        "The generated abstracts and rewritten articles will replace the existing content in the training data, providing better quality examples for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ OPENAI_API_KEY found\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "from utils import (\n",
        "    generate_abstracts_and_articles_batch,\n",
        "    load_conversations,\n",
        "    get_openai_client,\n",
        "    display_text,\n",
        "    display_message,\n",
        ")\n",
        "\n",
        "# Set OpenAI API key if not already set\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    print(\"Warning: OPENAI_API_KEY not set. Please set it before running.\")\n",
        "    print(\"You can set it with: os.environ['OPENAI_API_KEY'] = 'your-key-here'\")\n",
        "else:\n",
        "    print(\"✓ OPENAI_API_KEY found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: gpt-5\n",
            "Temperature: 1.0\n",
            "Max workers: 1000\n",
            "\n",
            "Train input: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct.jsonl\n",
            "Train output: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct_article_gpt5_v2.jsonl\n",
            "\n",
            "Val input: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct.jsonl\n",
            "Val output: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct_article_gpt5_v2.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL = \"gpt-5\"  # Use \"gpt-4o\", \"o1-preview\", or \"gpt-4-turbo\"\n",
        "TEMPERATURE = 1.0\n",
        "MAX_WORKERS = 1000  # Number of parallel API calls\n",
        "\n",
        "# File paths\n",
        "BASE_DIR = Path(\"/Users/ryanarman/code/lab/arxiv_abstract/data\")\n",
        "TRAIN_INPUT = BASE_DIR / \"arxiv_summarization_train_instruct.jsonl\"\n",
        "VAL_INPUT = BASE_DIR / \"arxiv_summarization_val_instruct.jsonl\"\n",
        "\n",
        "# Output paths\n",
        "TRAIN_OUTPUT = BASE_DIR / \"arxiv_summarization_train_instruct_article_gpt5_v2.jsonl\"\n",
        "VAL_OUTPUT = BASE_DIR / \"arxiv_summarization_val_instruct_article_gpt5_v2.jsonl\"\n",
        "\n",
        "print(f\"Model: {MODEL}\")\n",
        "print(f\"Temperature: {TEMPERATURE}\")\n",
        "print(f\"Max workers: {MAX_WORKERS}\")\n",
        "print(f\"\\nTrain input: {TRAIN_INPUT}\")\n",
        "print(f\"Train output: {TRAIN_OUTPUT}\")\n",
        "print(f\"\\nVal input: {VAL_INPUT}\")\n",
        "print(f\"Val output: {VAL_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Loaded 10000 training conversations\n",
            "\n",
            "Loading validation data...\n",
            "Loaded 1000 validation conversations\n"
          ]
        }
      ],
      "source": [
        "# Load training conversations\n",
        "print(\"Loading training data...\")\n",
        "train_conversations = load_conversations(TRAIN_INPUT)\n",
        "print(f\"Loaded {len(train_conversations)} training conversations\")\n",
        "\n",
        "# Load validation conversations\n",
        "print(\"\\nLoading validation data...\")\n",
        "val_conversations = load_conversations(VAL_INPUT)\n",
        "print(f\"Loaded {len(val_conversations)} validation conversations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_CONTENT_V1 = \"\"\"You are an expert academic abstract writer and scientific article editor. Your task has two parts:\n",
        "\n",
        "PART 1: Create a high-quality abstract for an arXiv paper based on the paper content.\n",
        "\n",
        "The judge evaluates abstracts based on five dimensions:\n",
        "1. Faithfulness: The abstract must accurately reflect the paper's content without hallucination\n",
        "2. Coverage: The abstract must include the essential aspects (main problem, approach, and key results)\n",
        "3. Clarity: The abstract must be understandable and readable\n",
        "4. Conciseness: The abstract must be focused and not verbose\n",
        "5. Coherence: The abstract must be logically structured and flow naturally\n",
        "\n",
        "PART 2: Rewrite the article content to create a clean, condensed version.\n",
        "\n",
        "The original article is noisy, contains placeholders for equations and figures, and is often too long. Your task is to:\n",
        "- Remove all placeholders (e.g., @xcite, @xmath, @xref, [figure X], [equation Y], etc.)\n",
        "- Clean up noisy text and formatting issues\n",
        "- Condense the content by focusing on the most important sections: Introduction, Conclusion, and Discussion\n",
        "- Remove redundant or less critical sections (detailed methodology, extensive background, etc.)\n",
        "- Maintain the core scientific content and key findings\n",
        "- Ensure the rewritten article is coherent and readable\n",
        "- Keep the article length reasonable (typically 30-50% of the original length)\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Your response must be formatted exactly as follows:\n",
        "\n",
        "ABSTRACT:\n",
        "[Your generated abstract here]\n",
        "\n",
        "ARTICLE:\n",
        "[Your rewritten article content here]\n",
        "\n",
        "Important: Start with \"ABSTRACT:\" on its own line, followed by the abstract. Then include \"ARTICLE:\" on its own line, followed by the rewritten article.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_CONTENT = \"\"\"\n",
        "You are an expert scientific editor and abstract writer. Your job is to convert a noisy scientific paper into a high-quality training example for supervised fine-tuning of a language model. Your output must contain two parts, in the exact order below.\n",
        "\n",
        "PART 1 — REWRITE THE ARTICLE (DO THIS FIRST)\n",
        "\n",
        "Rewrite the original paper into a clean, well-structured, information-preserving article that is suitable as the input to an abstract-generation model.\n",
        "\n",
        "GOALS:\n",
        "- Preserve all core scientific content: motivation, problem statement, assumptions, methodology, main results, contributions, implications, and conclusions.\n",
        "- Remove proofs, derivations, excessive notation, equation placeholders, citation placeholders (@xcite, @xmath, @xref), LaTeX artifacts, section cross-references, figure/table references, and appendices.\n",
        "- Summarize technical sections using plain-language descriptions rather than formulas or symbolic notation.\n",
        "- Maintain all ideas necessary for the abstract; do NOT remove concepts that the abstract would rely on.\n",
        "- Produce a coherent, readable academic narrative organized into the following sections:\n",
        "  1. Introduction\n",
        "  2. Problem Setting\n",
        "  3. Approach / Methodology\n",
        "  4. Main Results\n",
        "  5. Discussion and Implications\n",
        "  6. Conclusion\n",
        "- The rewritten article should be approximately 50%–70% of the original length.\n",
        "- Prioritize clarity, readability, and conceptual explanation over technical detail.\n",
        "\n",
        "CRITICAL RULE:\n",
        "Everything used in the abstract must appear explicitly in the rewritten article. When writing the abstract, use only the rewritten article as your source.\n",
        "\n",
        "PART 2 — WRITE THE ABSTRACT (SECOND, BASED ONLY ON THE REWRITTEN ARTICLE)\n",
        "\n",
        "Write a high-quality academic abstract using only the rewritten article from Part 1. Do NOT use the original noisy paper. Do NOT hallucinate or infer missing details.\n",
        "\n",
        "ABSTRACT REQUIREMENTS:\n",
        "1. Faithfulness: Must follow directly from the rewritten article.\n",
        "2. Coverage: Include problem, approach, main results, and contributions.\n",
        "3. Clarity: Use clear, accessible academic prose.\n",
        "4. Conciseness: Target 120–180 words, focused and non-redundant.\n",
        "5. Coherence: Logical flow across 4–7 sentences.\n",
        "\n",
        "Allowed: paraphrasing, summarizing, reorganizing ideas.\n",
        "Forbidden: adding new information, guessing missing equations/results, using external knowledge.\n",
        "\n",
        "OUTPUT FORMAT (REQUIRED):\n",
        "\n",
        "ARTICLE:\n",
        "[Rewritten article here]\n",
        "\n",
        "ABSTRACT:\n",
        "[Abstract here]\n",
        "\n",
        "Do not add explanations or reasoning traces. Only produce the ARTICLE and ABSTRACT sections in exactly this format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 10000 training conversations for generation\n",
            "Prepared 1000 validation conversations for generation\n"
          ]
        }
      ],
      "source": [
        "def prepare_conversations_for_generation(conversations):\n",
        "    \"\"\"\n",
        "    Prepare conversations for abstract and article generation.\n",
        "    Removes assistant messages and keeps only system + user (paper content).\n",
        "    Updates system message to include instructions for both abstract and article generation.\n",
        "    \"\"\"\n",
        "    prepared = []\n",
        "    for conv in conversations:\n",
        "        # Keep only system and user messages\n",
        "        filtered = [msg for msg in conv if msg.get('role') in ['system', 'user']]\n",
        "        if len(filtered) >= 2:  # Need at least system and user\n",
        "            # Update system message to include article rewriting instructions\n",
        "            updated_conv = []\n",
        "            for msg in filtered:\n",
        "                if msg.get('role') == 'system':\n",
        "                    # Update system message with combined instructions\n",
        "                    updated_conv.append({\n",
        "                        'role': 'system',\n",
        "                        'content': SYSTEM_CONTENT\n",
        "                    })\n",
        "                else:\n",
        "                    updated_conv.append(msg)\n",
        "            prepared.append(updated_conv)\n",
        "    return prepared\n",
        "\n",
        "# Prepare training conversations\n",
        "train_prepared = prepare_conversations_for_generation(train_conversations)\n",
        "print(f\"Prepared {len(train_prepared)} training conversations for generation\")\n",
        "\n",
        "# Prepare validation conversations\n",
        "val_prepared = prepare_conversations_for_generation(val_conversations)\n",
        "print(f\"Prepared {len(val_prepared)} validation conversations for generation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample prepared conversation structure:\n",
            "Number of messages: 2\n",
            "\n",
            "Message 1 - Role: system\n",
            "Content length: 2523 characters\n",
            "First 200 chars of system message:\n",
            "\n",
            "You are an expert scientific editor and abstract writer. Your job is to convert a noisy scientific paper into a high-quality training example for supervised fine-tuning of a language model. Your outp...\n",
            "\n",
            "Message 2 - Role: user\n",
            "Content length: 26760 characters\n"
          ]
        }
      ],
      "source": [
        "# Display a sample prepared conversation\n",
        "print(\"Sample prepared conversation structure:\")\n",
        "print(f\"Number of messages: {len(train_prepared[0])}\")\n",
        "for i, msg in enumerate(train_prepared[0]):\n",
        "    print(f\"\\nMessage {i+1} - Role: {msg['role']}\")\n",
        "    print(f\"Content length: {len(msg['content'])} characters\")\n",
        "    if msg['role'] == 'system':\n",
        "        print(\"First 200 chars of system message:\")\n",
        "        print(msg['content'][:200] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System instruction (first 500 characters):\n",
            "================================================================================\n",
            "\n",
            "You are an expert scientific editor and abstract writer. Your job is to convert a noisy scientific paper into a high-quality training example for supervised fine-tuning of a language model. Your output must contain two parts, in the exact order below.\n",
            "\n",
            "PART 1 — REWRITE THE ARTICLE (DO THIS FIRST)\n",
            "\n",
            "Rewrite the original paper into a clean, well-structured, information-preserving article that is suitable as the input to an abstract-generation model.\n",
            "\n",
            "GOALS:\n",
            "- Preserve all core scientific content: ...\n",
            "================================================================================\n",
            "\n",
            "Full system instruction length: 2523 characters\n"
          ]
        }
      ],
      "source": [
        "# Display system instruction (first 500 chars)\n",
        "print(\"System instruction (first 500 characters):\")\n",
        "print(\"=\"*80)\n",
        "print(train_prepared[0][0]['content'][:500] + \"...\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFull system instruction length: {len(train_prepared[0][0]['content'])} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GENERATING ABSTRACTS AND ARTICLES FOR TRAINING SET\n",
            "================================================================================\n",
            "Generating abstracts and articles for 10000 papers with 1000 workers...\n",
            "Using model: gpt-5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating abstracts and articles: 100%|██████████| 10000/10000 [18:28<00:00,  9.02paper/s, success=1e+4, errors=0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Completed: 10000 successful, 0 errors\n",
            "\n",
            "Training set: 10000 successful, 0 errors\n"
          ]
        }
      ],
      "source": [
        "# Generate abstracts and articles for training set\n",
        "print(\"=\"*80)\n",
        "print(\"GENERATING ABSTRACTS AND ARTICLES FOR TRAINING SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_results, train_errors = generate_abstracts_and_articles_batch(\n",
        "    train_prepared,\n",
        "    model=MODEL,\n",
        "    temperature=TEMPERATURE,\n",
        "    max_workers=MAX_WORKERS,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {len(train_results)} successful, {len(train_errors)} errors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SAMPLE GENERATED ABSTRACT\n",
            "================================================================================\n",
            "================================================================================\n",
            "Characters: 1,322 | Words: 185 | Lines: 1\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space: pre-wrap; word-wrap: break-word; font-family: monospace; background-color: #f5f5f5; color: #000000; padding: 10px; border-radius: 5px; line-height: 1.5;\">This paper studies support vector machines (SVMs) for additive models under\n",
              "convex, Lipschitz-continuous losses and smoothness penalties. We ask whether\n",
              "SVMs with additive kernels can learn substantially faster than SVMs with\n",
              "standard non-additive kernels when the additive assumption holds. Working in an\n",
              "RKHS built from a sum of component kernels, we derive high-probability excess-\n",
              "risk rates of the form n^(−α+ε) based on three ingredients: (i) an additive\n",
              "approximation condition expressed via integral-operator powers on the\n",
              "components, (ii) a new capacity bound showing that empirical covering-number\n",
              "exponents for the additive RKHS are independent of the number of components, and\n",
              "(iii) a variance–expectation noise condition. A key novelty is an approximation\n",
              "argument tailored to additive decompositions that circumvents the lack of a\n",
              "product-space risk. For quantile regression with the pinball loss, assuming\n",
              "bounded outputs, mild kernel regularity, r=1/2 approximation, and a τ-quantile\n",
              "of average type, we obtain rates whose exponent is independent of the quantile\n",
              "level, the number of components, and the ambient dimensions, approaching\n",
              "n^(−2/3) under favorable noise. Comparisons indicate that, in high dimensions,\n",
              "these additive-kernel rates outperform those for single-kernel SVMs on the full\n",
              "input space.</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SAMPLE GENERATED ARTICLE (first 1000 characters)\n",
            "================================================================================\n",
            "================================================================================\n",
            "Characters: 10,003 | Words: 1,519 | Lines: 35\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space: pre-wrap; word-wrap: break-word; font-family: monospace; background-color: #f5f5f5; color: #000000; padding: 10px; border-radius: 5px; line-height: 1.5;\">Introduction\n",
              "Additive models are widely used for semiparametric regression and classification\n",
              "because they increase flexibility relative to linear or generalized linear\n",
              "models while remaining more interpretable than fully nonparametric models. Good\n",
              "estimators in additive models are also typically less susceptible to the curse\n",
              "of dimensionality than estimators for fully nonparametric models. A large and\n",
              "successful class of estimators for such models are regularized kernel-based\n",
              "methods in reproducing kernel Hilbert spaces (RKHSs), often referred to as\n",
              "support vector machines (SVMs). While much of the existing theory on learning\n",
              "rates for additive models focuses on least squares loss and sparsity-inducing\n",
              "penalties, least squares is only locally Lipschitz and can be statistically non-\n",
              "robust. In contrast, SVMs with Lipschitz-continuous loss functions and bounded\n",
              "kernels admit robustness guarantees such as bounded influence and max-bias.\n",
              "\n",
              "This paper studies SVMs for additive models under general convex, Lipschitz-\n",
              "continuous loss functions and classical smoothness penalties (squared RKHS\n",
              "norm), not sparsity penalties. We address the core question of whether SVMs with\n",
              "additive kernels can achieve substantially better learning rates in high\n",
              "dimensions than SVMs with standard, non-additive kernels (e.g., Gaussian radial\n",
              "basis function kernels), when the additive model assumption holds. Our leading\n",
              "application is quantile regression using the Lipschitz but non-differentiable\n",
              "pinball (check) loss. We focus on learning rates and robustness; we do not\n",
              "address model checking for additivity or sparsity selection.\n",
              "\n",
              "Problem Setting\n",
              "We consider prediction from inputs X to outputs Y, where X is a complete\n",
              "separable metric space and Y is a closed subset of the real line. A Borel\n",
              "probability measure on X×Y defines the learning problem, and an i.i.d. sample is\n",
              "observed. A loss function L(x, y, t) measures prediction error. Throughout, L is\n",
              "measurable, convex in t, and uniformly Lipschitz in t with finite Lipschitz\n",
              "constant.\n",
              "\n",
              "SVMs are regularized kernel methods in an RKHS H generated by a Mercer kernel k.\n",
              "To accommodate heavy-tailed distributions, we work with a shifted loss that\n",
              "subtracts L(x, y, 0); minimizing the expected shifted loss plus a squared RKHS\n",
              "norm penalty is equivalent to minimizing the original expected loss plus the\n",
              "same penalty when a minimizer exists. The output of the learning algorithm is\n",
              "the minimizer of the regularized population objective, and its empirical\n",
              "counterpart is obtained by replacing the distribution with the empirical\n",
              "measure.\n",
              "\n",
              "We focus on additive models with the following structure. The input decomposes\n",
              "as a Cartesian product X = X1 × … × Xs. The hypothesis space is additive: H = H1\n",
              "+ … + Hs, where each Hj is an RKHS over Xj with kernel kj. The additive kernel\n",
              "is k = k1 + … + ks. The corresponding RKHS H comprises functions that are sums\n",
              "of component functions fj(xj) from Hj. The norm in H is the minimal squared sum\n",
              "of component norms among all such decompositions. This additive structure is\n",
              "central to our analysis.\n",
              "\n",
              "Two examples highlight advantages of additive over product kernels:\n",
              "- Gaussian RBF kernels: With an additive kernel given by the sum of univariate\n",
              "Gaussian kernels across coordinates, a function depending only on a single\n",
              "coordinate naturally lies in the additive RKHS (often with small norm), whereas\n",
              "it may fail to lie in the RKHS of the standard multivariate Gaussian product\n",
              "kernel defined on the full space. This indicates a potential approximation\n",
              "advantage for additive kernels.\n",
              "- Sobolev kernels: The univariate Sobolev space of square-integrable functions\n",
              "with square-integrable derivative is an RKHS with an associated Mercer kernel.\n",
              "Summing these univariate kernels yields a multivariate additive kernel whose\n",
              "RKHS remains well-behaved. In contrast, the full multivariate Sobolev space of\n",
              "the same order contains discontinuous functions and is not an RKHS, underscoring\n",
              "the structural benefit of the additive construction.\n",
              "\n",
              "Approach / Methodology\n",
              "We analyze excess risk, defined as the gap between the risk of the learned\n",
              "function and the Bayes optimal risk, and derive learning rates that quantify how\n",
              "this gap decays with the sample size. Our results apply to SVMs with convex\n",
              "Lipschitz loss and additive kernels under three types of conditions:\n",
              "approximation, capacity, and noise/variance.\n",
              "\n",
              "1) Approximation condition. Let f* denote a risk minimizer (Bayes decision\n",
              "function). The approximation error measures how well the hypothesis space H\n",
              "approximates f* under regularization; it is the excess risk of the population\n",
              "regularized minimizer. To control this error in the additive setting, we assume\n",
              "an additive structure for f*: there exist component functions f*_j on Xj such\n",
              "that f* = Σj f*_j, and each f*_j lies in a smoothness class defined via the\n",
              "integral operator associated with kj and the marginal measure on Xj.\n",
              "Specifically, each component is assumed to be in a range space of a positive\n",
              "power of this operator; the special case corresponding to power 1/2 places f*_j\n",
              "directly in Hj. A technical challenge arises because the risk is defined only on\n",
              "X, not on the product of marginal spaces. We address this by constructing\n",
              "intermediate approximants for the component functions and then summing them,\n",
              "which yields a tight bound on the approximation error in the additive RKHS. This\n",
              "construction is one of our main methodological novelties. Under this assumption,\n",
              "the approximation error decays polynomially in the regularization parameter,\n",
              "with a rate determined by the operator power.\n",
              "\n",
              "2) Capacity condition. We measure capacity via empirical L2 covering numbers of\n",
              "unit balls. Assume that for each component RKHS Hj the empirical covering\n",
              "numbers grow at most polynomially in inverse accuracy with exponent ζ. We prove\n",
              "that, thanks to additivity, the empirical covering numbers of the unit ball of\n",
              "the additive RKHS H inherit the same power exponent ζ. Crucially, this exponent\n",
              "does not depend on the number of additive components s. This dimension-\n",
              "independence contrasts with product-kernel or full-space Sobolev settings, where\n",
              "covering number exponents typically grow linearly with dimension. Establishing\n",
              "this dimension-independent capacity bound is the second key novelty of the\n",
              "paper.\n",
              "\n",
              "3) Noise/variance condition. We assume a variance–expectation bound of the form\n",
              "“variance of the loss difference is bounded by a constant times a power of its\n",
              "expected value,” with exponent θ in (0,1] and a finite constant. This condition\n",
              "always holds with θ = 1 but can be improved (larger θ) under benign noise,\n",
              "yielding faster rates. For quantile regression with the pinball loss, a standard\n",
              "“quantile of average type” condition on the conditional distribution of Y|X=x\n",
              "implies such a variance–expectation bound with favorable θ.\n",
              "\n",
              "Quantile regression specialization. We study SVMs with the pinball loss for a\n",
              "fixed quantile level τ. The target is the conditional τ-quantile function. A\n",
              "distribution has a τ-quantile of type q if the conditional distribution around\n",
              "the quantile satisfies lower mass conditions that ensure uniqueness and a\n",
              "margin-like property; an “average type q” condition imposes this almost surely\n",
              "in x with a mild integrability constraint. This condition is satisfied, for\n",
              "example, when the conditional density is bounded away from zero in a\n",
              "neighborhood of the quantile, which covers many familiar parametric families.\n",
              "Under bounded outputs and mild regularity of the kernels, we derive explicit\n",
              "learning rates for the pinball loss in the additive RKHS.\n",
              "\n",
              "Main Results\n",
              "General learning rates. Under (i) the additive approximation assumption\n",
              "described above (with exponent r from the operator power), (ii) the\n",
              "componentwise capacity assumption with exponent ζ and its dimension-independent\n",
              "transfer to the additive RKHS, and (iii) the variance–expectation bound with\n",
              "exponent θ, choosing the regularization parameter as a power of the sample size\n",
              "yields high-probability excess risk bounds of the form n^(-α+ε), for any\n",
              "arbitrarily small ε>0. The exponent α is an explicit function of r, ζ, and θ.\n",
              "Importantly, α does not depend on the number of additive components s or on the\n",
              "ambient dimension. All constants in the bounds are independent of n, s, and the\n",
              "dimensions of the component spaces.\n",
              "\n",
              "Quantile regression rates. For the pinball loss, assume bounded outputs, bounded\n",
              "and sufficiently regular component kernels, and the additive approximation\n",
              "assumption with r=1/2 (i.e., each f*_j lies in Hj). If the conditional\n",
              "distributions have τ-quantiles of average type q, then with an appropriate\n",
              "choice of the regularization parameter we obtain high-probability excess risk\n",
              "bounds of the form n^(-α+ε), with α independent of the quantile level τ, the\n",
              "number of components s, and the dimensionalities of the components and of the\n",
              "full input. As the noise condition strengthens (increasing q), α approaches a\n",
              "value close to 2/3. Under weaker noise, α remains positive and can be close to\n",
              "1/2 or better, depending on θ.\n",
              "\n",
              "Comparisons. Our quantile regression learning rate is new for SVMs with additive\n",
              "kernels and does not rely on clipping (projection) of the estimator outputs,\n",
              "whereas many existing results do. Compared to essentially minimax-optimal rates\n",
              "derived for SVMs with a single Gaussian RBF kernel on the entire input space,\n",
              "our additive-kernel rates are substantially better in high dimensions when the\n",
              "additive model assumption holds. Prior rates with non-additive kernels\n",
              "deteriorate with dimension through capacity exponents or eigenvalue conditions;\n",
              "in contrast, our capacity exponent is dimension-independent, and the resulting\n",
              "learning rates do not degrade with increasing s or ambient dimension. Numerical\n",
              "illustrations in the original study (not reproduced here) indicate especially\n",
              "pronounced gains in high-dimensional settings, although small-dimensional\n",
              "optimality is not our primary focus.\n",
              "\n",
              "Further illustration. Using additive Gaussian kernels wi...</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Full article length: 12363 characters\n"
          ]
        }
      ],
      "source": [
        "# Display sample generated abstract and article\n",
        "idx, result = train_results[0]\n",
        "print(\"=\"*80)\n",
        "print(\"SAMPLE GENERATED ABSTRACT\")\n",
        "print(\"=\"*80)\n",
        "display_text(result['abstract'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE GENERATED ARTICLE (first 1000 characters)\")\n",
        "print(\"=\"*80)\n",
        "if result.get('article'):\n",
        "    display_text(result['article'][:10000] + \"...\" if len(result['article']) > 10000 else result['article'])\n",
        "    print(f\"\\nFull article length: {len(result['article'])} characters\")\n",
        "else:\n",
        "    print(\"No article generated (check if model response format was correct)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GENERATING ABSTRACTS AND ARTICLES FOR VALIDATION SET\n",
            "================================================================================\n",
            "Generating abstracts and articles for 1000 papers with 1000 workers...\n",
            "Using model: gpt-5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating abstracts and articles: 100%|██████████| 1000/1000 [03:29<00:00,  4.78paper/s, success=1000, errors=0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Completed: 1000 successful, 0 errors\n",
            "\n",
            "Validation set: 1000 successful, 0 errors\n"
          ]
        }
      ],
      "source": [
        "# Generate abstracts and articles for validation set\n",
        "print(\"=\"*80)\n",
        "print(\"GENERATING ABSTRACTS AND ARTICLES FOR VALIDATION SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "val_results, val_errors = generate_abstracts_and_articles_batch(\n",
        "    val_prepared,\n",
        "    model=MODEL,\n",
        "    temperature=TEMPERATURE,\n",
        "    max_workers=MAX_WORKERS,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"\\nValidation set: {len(val_results)} successful, {len(val_errors)} errors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SAMPLE GENERATED ABSTRACT (VALIDATION SET)\n",
            "================================================================================\n",
            "================================================================================\n",
            "Characters: 1,515 | Words: 195 | Lines: 3\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space: pre-wrap; word-wrap: break-word; font-family: monospace; background-color: #f5f5f5; color: #000000; padding: 10px; border-radius: 5px; line-height: 1.5;\">We study a degenerate, type-I optical parametric oscillator containing a\n",
              "transverse photonic crystal (PCOPO) and analyze how spatial modulation of cavity\n",
              "detunings shapes quantum fluctuations and correlations below threshold. Starting\n",
              "from a χ(2) Hamiltonian with damping and vacuum noise, we linearize around the\n",
              "steady state (vanishing signal mean) and construct a few-mode model that retains\n",
              "the dominant pump harmonics and the two critical signal modes. Using\n",
              "input–output theory, we derive analytical expressions for the instability\n",
              "threshold, signal intensity spectra, quadrature squeezing and entanglement, and\n",
              "twin-beam (intensity-difference) correlations. Full multimode stochastic\n",
              "simulations validate the approximations and reveal pattern locking due to broken\n",
              "translational symmetry.\n",
              "\n",
              "The photonic crystal enables threshold control: signal detuning modulation\n",
              "raises the threshold, while pump detuning modulation can lower it. At a fixed\n",
              "distance from threshold, squeezing, separability, and EPR entanglement are\n",
              "preserved relative to a homogeneous OPO. Twin-beam correlations, constant and\n",
              "nonclassical in the homogeneous case, become parameter dependent: signal\n",
              "modulation can degrade them to classical levels via modulation-induced hopping\n",
              "between the ± critical modes, whereas pump-only modulation preserves\n",
              "nonclassicality with slight reduction. These results demonstrate intracavity\n",
              "photonic crystals as effective tools for engineering spatial quantum\n",
              "correlations and operating at reduced energy.</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SAMPLE GENERATED ARTICLE (VALIDATION SET, first 1000 characters)\n",
            "================================================================================\n",
            "================================================================================\n",
            "Characters: 10,003 | Words: 1,377 | Lines: 57\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space: pre-wrap; word-wrap: break-word; font-family: monospace; background-color: #f5f5f5; color: #000000; padding: 10px; border-radius: 5px; line-height: 1.5;\">Introduction\n",
              "Quantum correlations between spatially or temporally separated components\n",
              "underpin quantum information processing, quantum cryptography, and quantum-\n",
              "enhanced metrology. Optical platforms operating in the continuous-variable,\n",
              "many-photon regime have proved especially successful, with the optical\n",
              "parametric oscillator (OPO) serving as a key source of squeezing and\n",
              "entanglement in field quadratures. Beyond polarization and frequency, spatial\n",
              "degrees of freedom provide a rich arena where multimode quantum correlations\n",
              "emerge between cavity modes or across different regions of a beam, enabling\n",
              "applications in switching, imaging, metrology, and information processing.\n",
              "\n",
              "Photonic crystals (PCs) offer structured control of light through periodic\n",
              "refractive index modulations that open photonic band-gaps, confine and guide\n",
              "light, and profoundly alter linear and nonlinear optical dynamics. In\n",
              "dissipative nonlinear cavities, transverse index modulation can inhibit\n",
              "modulation instabilities and support discrete cavity solitons. In quantum\n",
              "optics, PCs have been proposed to engineer dissipation and structured\n",
              "reservoirs, leading to non-Markovian dynamics and modified decoherence. Here we\n",
              "address a different, complementary use of PCs: intracavity spatial modulation to\n",
              "engineer quantum fluctuations and correlations of light.\n",
              "\n",
              "We analyze a photonic-crystal optical parametric oscillator (PCOPO): a\n",
              "degenerate, type-I OPO with a transverse photonic crystal inside the cavity.\n",
              "Prior numerical work showed that, compared with a homogeneous OPO, a PCOPO can\n",
              "tune quantum correlations by controlling spatial mode coupling, with improved\n",
              "squeezing, robustness across angles, and enhanced entanglement above threshold.\n",
              "In this paper we provide an analytical treatment valid below threshold,\n",
              "supported by numerical simulations, to quantify how the photonic crystal\n",
              "modifies intensity fluctuations, squeezing, entanglement, and twin-beam\n",
              "correlations.\n",
              "\n",
              "Problem Setting\n",
              "We consider a planar cavity containing a quadratic (second-order) nonlinear\n",
              "medium. A classical pump at the higher frequency is down-converted into a\n",
              "degenerate signal at half the frequency with orthogonal polarization (type-I\n",
              "OPO). The input is a plane wave along the cavity axis with real amplitude. The\n",
              "cavity is engineered with a photonic crystal that produces a transverse\n",
              "modulation of the refractive index, and hence of the cavity detunings. The\n",
              "modulation can affect the signal, the pump, or both, and breaks the transverse\n",
              "translational symmetry.\n",
              "\n",
              "Below the parametric instability threshold, the mean signal field vanishes while\n",
              "the pump acquires a spatial profile determined by the modulation. For analytical\n",
              "tractability we focus on one transverse dimension and a sinusoidal detuning\n",
              "modulation whose wave number is matched to couple the most unstable OPO modes.\n",
              "In a homogeneous OPO with negative signal detuning, modulation instability above\n",
              "threshold occurs at a critical transverse wave number; below threshold, noisy\n",
              "precursors at this wave number appear. We choose the photonic crystal\n",
              "periodicity to induce coupling between the two critical signal modes at ± the\n",
              "critical wave number. This choice creates a band-gap around those modes for the\n",
              "signal and induces even-harmonic content in the pump.\n",
              "\n",
              "Approach / Methodology\n",
              "We start from a standard χ(2) Hamiltonian including diffraction, coherent\n",
              "driving of the pump, and three-wave mixing, with cavity damping and vacuum input\n",
              "noise. The Heisenberg–Langevin equations describe the open-system dynamics. To\n",
              "obtain analytical results below threshold, we linearize around the steady state:\n",
              "the signal mean field is zero; the pump steady state is obtained by averaging\n",
              "the equations and retaining only the dominant spatial components generated by\n",
              "the photonic crystal. This yields a few-mode description in which the pump\n",
              "occupies the homogeneous mode and its first spatial harmonics, while the signal\n",
              "fluctuations are dominated by the two critical modes at ± the critical wave\n",
              "number.\n",
              "\n",
              "The photonic crystal couples these signal modes both directly (through a\n",
              "spatially varying signal detuning) and indirectly (via the spatially modulated\n",
              "pump). The resulting linear equations for the four signal fluctuation operators\n",
              "(the two critical modes and their conjugates) are solved in the frequency domain\n",
              "using the input–output formalism to obtain the output fields in terms of the\n",
              "input vacuum fluctuations.\n",
              "\n",
              "We compute analytical expressions for:\n",
              "- The signal intensity spectra and the parametric instability threshold as\n",
              "functions of the modulation amplitudes in the signal and pump detunings.\n",
              "- Quadrature squeezing spectra and continuous-variable entanglement measures\n",
              "(separability and EPR criteria) for superpositions of the ± critical modes.\n",
              "- Twin-beam (intensity-difference) correlations between the two critical modes,\n",
              "expressed as the variance normalized to shot noise.\n",
              "\n",
              "To assess the validity of the approximations, we simulate the full multimode\n",
              "nonlinear dynamics using a phase-space (Q) representation. This yields spatially\n",
              "dependent stochastic Langevin equations with additive (pump) and multiplicative,\n",
              "phase-sensitive (signal) white noises. Numerical integration across parameters\n",
              "allows direct comparison with the few-mode analytical predictions.\n",
              "\n",
              "Main Results\n",
              "Mode content and pattern locking\n",
              "Numerically, in the homogeneous OPO the signal shows noisy precursors at the\n",
              "critical spatial frequency whose phase diffuses across the transverse dimension\n",
              "due to a Goldstone mode associated with translational symmetry. In contrast, in\n",
              "the PCOPO the photonic crystal locks the spatial phase: two dephased critical\n",
              "modes dominate and the noisy pattern is pinned, reflecting the broken\n",
              "translational symmetry. The pump develops even spatial harmonics when its\n",
              "detuning is modulated, while the average signal far-field intensity remains\n",
              "unchanged by the modulation.\n",
              "\n",
              "Validity of the few-mode approximation\n",
              "The pump spectrum is dominated by the homogeneous component and the first even\n",
              "harmonic; higher-order harmonics are much weaker and can be neglected. The\n",
              "signal is dominated by the two critical modes at ± the critical wave number.\n",
              "This justifies the truncation to a small set of modes in the analytical\n",
              "treatment and explains the good agreement with the full simulations below\n",
              "threshold.\n",
              "\n",
              "Threshold control\n",
              "The photonic crystal modifies the parametric instability threshold:\n",
              "- Modulating the signal detuning generally raises the threshold, inhibiting the\n",
              "instability (pattern inhibition).\n",
              "- Modulating the pump detuning can lower the threshold, favoring the\n",
              "instability.\n",
              "- When both fields are modulated, the net effect depends on parameter choices;\n",
              "both threshold increase and decrease are possible.\n",
              "The analytical threshold conditions derived from the linearized model match the\n",
              "numerical results.\n",
              "\n",
              "Squeezing and entanglement\n",
              "At a fixed distance from threshold (i.e., comparing OPO and PCOPO under equal\n",
              "relative pump-to-threshold conditions), quadrature squeezing, separability, and\n",
              "EPR entanglement are preserved in both the attainable values and in the angular\n",
              "ranges (quadrature and mixing angles) over which they occur. Because pump\n",
              "modulation can reduce the threshold, the same absolute pump power places the\n",
              "PCOPO closer to threshold, thereby improving correlations relative to the\n",
              "homogeneous OPO at equal pump power.\n",
              "\n",
              "Twin-beam correlations\n",
              "In the homogeneous OPO, the normalized intensity-difference variance between the\n",
              "± critical modes is constant, negative (nonclassical), and independent of pump\n",
              "strength below threshold. In the PCOPO:\n",
              "- With signal detuning modulation, the twin-beam correlations degrade with\n",
              "increasing modulation and can become classical (variance above shot noise) even\n",
              "below threshold. Microscopically, the spatial modulation couples the two\n",
              "critical modes, enabling processes that create a photon in one mode while\n",
              "annihilating a photon in the opposite mode. This “hopping” depletes the paired-\n",
              "photon correlation underlying twin beams.\n",
              "- With pump detuning modulation only, nonclassical twin-beam correlations\n",
              "persist for all pump strengths below threshold, although their strength is\n",
              "slightly reduced due to secondary processes triggered by coupling among pump\n",
              "harmonics.\n",
              "These behaviors are captured by the analytical expressions and confirmed by\n",
              "simulations.\n",
              "\n",
              "Discussion and Implications\n",
              "Intracavity photonic crystals provide a powerful means to engineer spatial\n",
              "quantum correlations in multimode nonlinear cavities. By selecting which\n",
              "detuning (signal, pump, or both) is modulated and by tuning the modulation\n",
              "depth, one can:\n",
              "- Raise or lower the parametric threshold, enabling operation at lower energy\n",
              "for a given level of quantum correlations or, conversely, suppressing\n",
              "instabilities.\n",
              "- Preserve squeezing and entanglement at a fixed distance from threshold, with\n",
              "the potential to enhance them at a given pump power when the threshold is\n",
              "lowered.\n",
              "- Tailor intensity correlations: signal modulation can intentionally suppress\n",
              "twin-beam correlations, whereas pump-only modulation maintains their\n",
              "nonclassicality with modest degradation.\n",
              "\n",
              "The mechanisms identified—mode coupling, pattern locking, and modulation-induced\n",
              "hopping—are generic and can be extended to other multimode nonlinear devices\n",
              "such as Kerr media and second-harmonic generation cavities modified by\n",
              "intracavity photonic crystals.\n",
              "\n",
              "Conclusion\n",
              "We presented an analytical, few-mode theory of a photonic-crystal OPO below\n",
              "threshold, supported by full nonlinear stochastic simulations. The photonic\n",
              "crystal breaks translational symmetry, locks noisy spatial precursors, and\n",
              "couples the critical modes, enabling controlled manipulation of the instability\n",
              "threshold and of key quantum correlations. Signal modulation raises the\n",
              "threshold and can suppress twin-beam correlations through mode hopping, whereas\n",
              "pump modulation can lower the threshold and preserves nonclassical intensity\n",
              "correlations while maintaining squeezing and entanglement...</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Full article length: 10216 characters\n"
          ]
        }
      ],
      "source": [
        "# Display sample generated abstract and article from validation set\n",
        "idx, result = val_results[0]\n",
        "print(\"=\"*80)\n",
        "print(\"SAMPLE GENERATED ABSTRACT (VALIDATION SET)\")\n",
        "print(\"=\"*80)\n",
        "display_text(result['abstract'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE GENERATED ARTICLE (VALIDATION SET, first 1000 characters)\")\n",
        "print(\"=\"*80)\n",
        "if result.get('article'):\n",
        "    display_text(result['article'][:10000] + \"...\" if len(result['article']) > 10000 else result['article'])\n",
        "    print(f\"\\nFull article length: {len(result['article'])} characters\")\n",
        "else:\n",
        "    print(\"No article generated (check if model response format was correct)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving training results...\n",
            "Saved 10000 training examples to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct_article_gpt5_v2.jsonl\n",
            "\n",
            "Saving validation results...\n",
            "Saved 1000 validation examples to /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct_article_gpt5_v2.jsonl\n"
          ]
        }
      ],
      "source": [
        "def save_generated_abstracts_and_articles(original_conversations, generation_results, output_path):\n",
        "    \"\"\"\n",
        "    Save generated abstracts and rewritten articles in the same format as training data.\n",
        "    \n",
        "    Args:\n",
        "        original_conversations: Original conversations with system/user messages\n",
        "        generation_results: List of (index, result_dict) tuples from generate_abstracts_and_articles_batch\n",
        "        output_path: Path to save the output JSONL file\n",
        "    \"\"\"\n",
        "    # Create mappings of index to generated abstract and article\n",
        "    abstract_map = {idx: result['abstract'] for idx, result in generation_results}\n",
        "    article_map = {idx: result.get('article', '') for idx, result in generation_results}\n",
        "    \n",
        "    saved_count = 0\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for idx, original_conv in enumerate(original_conversations):\n",
        "            if idx in abstract_map:\n",
        "                # Create new conversation with generated abstract and rewritten article\n",
        "                new_conv = []\n",
        "                \n",
        "                # Process original messages\n",
        "                for msg in original_conv:\n",
        "                    if msg.get('role') == 'user':\n",
        "                        # Replace user message with rewritten article if available\n",
        "                        if idx in article_map and article_map[idx]:\n",
        "                            new_conv.append({\n",
        "                                'role': 'user',\n",
        "                                'content': f\"Paper Content:\\n{article_map[idx]}\"\n",
        "                            })\n",
        "                        else:\n",
        "                            # Keep original user message if no article was generated\n",
        "                            new_conv.append(msg)\n",
        "                    elif msg.get('role') == 'system':\n",
        "                        # Keep system message (it should already have the updated instructions)\n",
        "                        new_conv.append(msg)\n",
        "                    elif msg.get('role') == 'assistant':\n",
        "                        # Skip original assistant message, we'll add the new one\n",
        "                        pass\n",
        "                \n",
        "                # Add generated abstract as assistant message\n",
        "                new_conv.append({\n",
        "                    'role': 'assistant',\n",
        "                    'content': abstract_map[idx]\n",
        "                })\n",
        "                \n",
        "                # Write to file\n",
        "                json.dump({'messages': new_conv}, f, ensure_ascii=False)\n",
        "                f.write('\\n')\n",
        "                saved_count += 1\n",
        "            else:\n",
        "                # If generation failed, skip this example\n",
        "                print(f\"Warning: Skipping index {idx} (generation failed)\")\n",
        "    \n",
        "    return saved_count\n",
        "\n",
        "# Save training results\n",
        "print(\"Saving training results...\")\n",
        "train_saved = save_generated_abstracts_and_articles(train_conversations, train_results, TRAIN_OUTPUT)\n",
        "print(f\"Saved {train_saved} training examples to {TRAIN_OUTPUT}\")\n",
        "\n",
        "# Save validation results\n",
        "print(\"\\nSaving validation results...\")\n",
        "val_saved = save_generated_abstracts_and_articles(val_conversations, val_results, VAL_OUTPUT)\n",
        "print(f\"Saved {val_saved} validation examples to {VAL_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Role: ASSISTANT\n",
            "Characters: 1,553 | Words: 210 | Lines: 1\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space: pre-wrap; word-wrap: break-word; font-family: monospace; background-color: #f5f5f5; color: #000000; padding: 10px; border-radius: 5px; line-height: 1.5;\">We study support vector machines (SVMs) for additive models with general convex,\n",
              "Lipschitz continuous losses and RKHS norm regularization. Motivated by\n",
              "robustness and the curse of dimensionality, we address whether SVMs using\n",
              "additive kernels achieve substantially faster learning in high dimensions than\n",
              "SVMs with standard (non-additive) kernels when the additive model assumption\n",
              "holds. Our analysis establishes finite-sample excess risk rates of order n^{-α}\n",
              "(up to logarithmic factors) for additive-kernel SVMs, where the exponent α is\n",
              "independent of the number of additive components and their dimensions. The\n",
              "results are derived under three conditions: (i) an additive approximation\n",
              "condition formulated via powers of integral operators, handled by a new\n",
              "intermediate-function technique; (ii) dimension-independent capacity bounds,\n",
              "showing empirical covering numbers of additive RKHS balls retain the single-\n",
              "component power exponent; and (iii) a variance–expectation (noise) condition\n",
              "covering heavy-tailed settings via loss shifting. A leading application is\n",
              "quantile regression with the pinball loss, for which we obtain new learning\n",
              "rates whose exponents are independent of the quantile level and the ambient\n",
              "dimension. Examples with Gaussian RBF and Sobolev kernels illustrate\n",
              "approximation advantages of additive kernels. Theoretical and numerical\n",
              "comparisons show that, when the additive structure is valid, additive-kernel\n",
              "SVMs can significantly outperform single-kernel SVMs (e.g., Gaussian RBF on the\n",
              "full input space) in high dimensions.</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Verify saved results\n",
        "gen_train_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct_article_gpt5.jsonl\"\n",
        "gen_train_conversations = load_conversations(gen_train_path)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"VERIFYING SAVED RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Loaded {len(gen_train_conversations)} conversations from saved file\")\n",
        "print(f\"\\nFirst conversation has {len(gen_train_conversations[0])} messages\")\n",
        "for i, msg in enumerate(gen_train_conversations[0]):\n",
        "    print(f\"  Message {i+1}: role={msg['role']}, length={len(msg['content'])} chars\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE GENERATED ABSTRACT (from saved file)\")\n",
        "print(\"=\"*80)\n",
        "display_message(gen_train_conversations[0], \"assistant\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE REWRITTEN ARTICLE (from saved file, first 500 chars)\")\n",
        "print(\"=\"*80)\n",
        "user_msg = None\n",
        "for msg in gen_train_conversations[0]:\n",
        "    if msg['role'] == 'user':\n",
        "        user_msg = msg['content']\n",
        "        break\n",
        "if user_msg:\n",
        "    # Remove \"Paper Content:\\n\" prefix if present\n",
        "    article_content = user_msg.replace(\"Paper Content:\\n\", \"\", 1)\n",
        "    display_text(article_content[:500] + \"...\" if len(article_content) > 500 else article_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Role: ASSISTANT\n",
            "Characters: 1,597 | Words: 215 | Lines: 1\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space: pre-wrap; word-wrap: break-word; font-family: monospace; background-color: #f5f5f5; color: #000000; padding: 10px; border-radius: 5px; line-height: 1.5;\">We study how a transverse photonic crystal (PC) embedded in a planar, type‑I\n",
              "degenerate optical parametric oscillator (OPO) controls spatial quantum\n",
              "fluctuations and correlations. Modeling the PC as a sinusoidal modulation of the\n",
              "intracavity detunings, we linearize the quantum dynamics below threshold and\n",
              "develop a few‑mode description that couples the critical signal modes ±kc to\n",
              "pump harmonics induced by the PC. Using input–output theory, we derive\n",
              "analytical expressions for intensity spectra, quadrature squeezing,\n",
              "separability/EPR entanglement, and twin‑beam (intensity‑difference)\n",
              "correlations. The analysis predicts that the modulation breaks translational\n",
              "symmetry and locks the noisy precursors of pattern formation. Crucially, the\n",
              "parametric instability threshold can be raised or lowered depending on whether\n",
              "the PC acts on the signal, the pump, or both: signal‑only modulation tends to\n",
              "inhibit the instability, whereas appropriate pump modulation can reduce the\n",
              "threshold. At a fixed distance from threshold, the degree and angular robustness\n",
              "of squeezing and entanglement are essentially preserved relative to a\n",
              "homogeneous OPO, while twin‑beam correlations are more sensitive to PC‑induced\n",
              "mode coupling: signal modulation degrades and can render them classical, whereas\n",
              "pump‑only modulation maintains their quantum character with only minor\n",
              "reduction. All predictions agree well with numerical simulations of the full\n",
              "multimode Langevin equations. These results show that intracavity PCs enable\n",
              "energy‑efficient and flexible control of spatial continuous‑variable quantum\n",
              "light.</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Verify saved validation results\n",
        "gen_val_path = \"/Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct_article_gpt5.jsonl\"\n",
        "gen_val_conversations = load_conversations(gen_val_path)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"VERIFYING SAVED VALIDATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Loaded {len(gen_val_conversations)} conversations from saved file\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE GENERATED ABSTRACT (VALIDATION, from saved file)\")\n",
        "print(\"=\"*80)\n",
        "display_message(gen_val_conversations[0], \"assistant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GENERATION SUMMARY\n",
            "================================================================================\n",
            "Model used: gpt-5\n",
            "\n",
            "Training set:\n",
            "  Generated: 10000 abstracts\n",
            "  Errors: 0\n",
            "  Saved to: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_train_instruct_gpt5.jsonl\n",
            "\n",
            "Validation set:\n",
            "  Generated: 1000 abstracts\n",
            "  Errors: 0\n",
            "  Saved to: /Users/ryanarman/code/lab/arxiv_abstract/data/arxiv_summarization_val_instruct_gpt5.jsonl\n",
            "\n",
            "================================================================================\n",
            "SAMPLE GENERATED ABSTRACT\n",
            "================================================================================\n",
            "\n",
            "Index 0:\n",
            "Additive models offer flexibility and interpretability while mitigating the curse of dimensionality. We study support vector machines (SVMs) with additive kernels in reproducing kernel Hilbert spaces under general convex, Lipschitz continuous losses with Tikhonov regularization, addressing the open question of whether additive kernels yield substantially faster learning in high dimensions when the additive assumption holds. Our analysis covers heavy-tailed distributions via a shifted loss and do...\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"GENERATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model used: {MODEL}\")\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  Generated: {len(train_results)} abstracts and articles\")\n",
        "print(f\"  Errors: {len(train_errors)}\")\n",
        "print(f\"  Saved to: {TRAIN_OUTPUT}\")\n",
        "print(f\"\\nValidation set:\")\n",
        "print(f\"  Generated: {len(val_results)} abstracts and articles\")\n",
        "print(f\"  Errors: {len(val_errors)}\")\n",
        "print(f\"  Saved to: {VAL_OUTPUT}\")\n",
        "\n",
        "# Show statistics about generated content\n",
        "if train_results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GENERATION STATISTICS\")\n",
        "    print(\"=\"*80)\n",
        "    abstracts_with_articles = sum(1 for _, result in train_results if result.get('article'))\n",
        "    avg_abstract_len = sum(len(result['abstract']) for _, result in train_results) / len(train_results)\n",
        "    avg_article_len = sum(len(result.get('article', '')) for _, result in train_results) / len(train_results)\n",
        "    \n",
        "    print(f\"Abstracts generated: {len(train_results)}\")\n",
        "    print(f\"Articles generated: {abstracts_with_articles}\")\n",
        "    print(f\"Average abstract length: {avg_abstract_len:.0f} characters\")\n",
        "    print(f\"Average article length: {avg_article_len:.0f} characters\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAMPLE GENERATED ABSTRACT\")\n",
        "    print(\"=\"*80)\n",
        "    idx, result = train_results[0]\n",
        "    print(f\"\\nIndex {idx}:\")\n",
        "    print(result['abstract'][:500] + \"...\" if len(result['abstract']) > 500 else result['abstract'])\n",
        "    \n",
        "    if result.get('article'):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"SAMPLE GENERATED ARTICLE (first 500 chars)\")\n",
        "        print(\"=\"*80)\n",
        "        print(result['article'][:500] + \"...\" if len(result['article']) > 500 else result['article'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oumi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
